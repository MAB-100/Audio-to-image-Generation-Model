{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"388de842b7a644b4a0869d0689bc8e9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92b3d9052dca4b6cbc972b150543d131","IPY_MODEL_547da6ff19924beb8b8e4baf6977c916","IPY_MODEL_f62243cfa3af4cd3bb7016cbe5668949"],"layout":"IPY_MODEL_0b03526485314680950c7b2d5bf64a9f"}},"92b3d9052dca4b6cbc972b150543d131":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5163cc045eb94813b1884dc817e4dc5e","placeholder":"​","style":"IPY_MODEL_c177517a63e44721ac50b38987e6c8ac","value":"Loading pipeline components...: 100%"}},"547da6ff19924beb8b8e4baf6977c916":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc1f56bcf0f406cb0890ee3ee9b4bc2","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eaad4d69f208441bb97499f857ff5a40","value":7}},"f62243cfa3af4cd3bb7016cbe5668949":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_714f40149efe417ba8d4526f89ce2e1b","placeholder":"​","style":"IPY_MODEL_d07e1c359d3f4f52a88df737ddc9eb45","value":" 7/7 [00:02&lt;00:00,  2.50it/s]"}},"0b03526485314680950c7b2d5bf64a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5163cc045eb94813b1884dc817e4dc5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c177517a63e44721ac50b38987e6c8ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fc1f56bcf0f406cb0890ee3ee9b4bc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaad4d69f208441bb97499f857ff5a40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"714f40149efe417ba8d4526f89ce2e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d07e1c359d3f4f52a88df737ddc9eb45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3af97dcc70d45f6bc01aeaabc1b7873":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47586a87e4c44574982bd27e48394e51","IPY_MODEL_c8d398ed8470420db7e7efbc78cd1af1","IPY_MODEL_c5645182e0714f0880f704e300c2f5f4"],"layout":"IPY_MODEL_9c49f2c23bf44a93b403e2a1145aae3e"}},"47586a87e4c44574982bd27e48394e51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ef2f2771bb84b859998eec25c0ab9c7","placeholder":"​","style":"IPY_MODEL_d91951f2eb1f459682aee7d78d7bda06","value":"100%"}},"c8d398ed8470420db7e7efbc78cd1af1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c3842b2dbe54b788acabb46665f6a4f","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6be2f7ccbee948659bbfef40e937614f","value":50}},"c5645182e0714f0880f704e300c2f5f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8858a8968024442b5637d676f14bb40","placeholder":"​","style":"IPY_MODEL_26a08cde44914b778aceb3489c8ea276","value":" 50/50 [00:50&lt;00:00,  1.01s/it]"}},"9c49f2c23bf44a93b403e2a1145aae3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ef2f2771bb84b859998eec25c0ab9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91951f2eb1f459682aee7d78d7bda06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c3842b2dbe54b788acabb46665f6a4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be2f7ccbee948659bbfef40e937614f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8858a8968024442b5637d676f14bb40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26a08cde44914b778aceb3489c8ea276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c787f8ade93946c8a4eb4983c37e7d35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e06a3a9502243d5adaab4511a9bb164","IPY_MODEL_3de3da535dc348c999d22ee5469685ac","IPY_MODEL_4c5161a7a1d14ab3a03961dc51636af3"],"layout":"IPY_MODEL_dbe18c4f82e14421a6f6fb1b6bed5cc8"}},"5e06a3a9502243d5adaab4511a9bb164":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_953ba8e85ad84b01b7a3ebddfdcef530","placeholder":"​","style":"IPY_MODEL_57a9c29d312d438890324b064fb4d915","value":"Loading pipeline components...: 100%"}},"3de3da535dc348c999d22ee5469685ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6f91b9762784743a74602af46a7d44c","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a9de3c73c0742e387b8011ba3d4e06a","value":7}},"4c5161a7a1d14ab3a03961dc51636af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcb2bc6f2b8945949f9f84e8faaa1e95","placeholder":"​","style":"IPY_MODEL_a9f2f9cc136946a0a6eb29cb08b587b4","value":" 7/7 [00:01&lt;00:00,  3.28it/s]"}},"dbe18c4f82e14421a6f6fb1b6bed5cc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"953ba8e85ad84b01b7a3ebddfdcef530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57a9c29d312d438890324b064fb4d915":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6f91b9762784743a74602af46a7d44c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a9de3c73c0742e387b8011ba3d4e06a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcb2bc6f2b8945949f9f84e8faaa1e95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9f2f9cc136946a0a6eb29cb08b587b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10179163,"sourceType":"datasetVersion","datasetId":6287563},{"sourceId":196094,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":167199,"modelId":189517},{"sourceId":196871,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":167889,"modelId":190232}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# coding=utf-8\n\nimport argparse\nimport logging\nimport math\nimport os\nimport random\nimport shutil\nfrom contextlib import nullcontext\nfrom pathlib import Path\n\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import DistributedDataParallelKwargs, DistributedType, ProjectConfiguration, set_seed\nfrom datasets import load_dataset\nfrom huggingface_hub import create_repo, upload_folder\nfrom packaging import version\nfrom peft import LoraConfig, set_peft_model_state_dict\nfrom peft.utils import get_peft_model_state_dict\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, CLIPTextModel\n\nimport diffusers\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    StableDiffusionPipeline,\n    UNet2DConditionModel,\n)\nfrom diffusers.loaders import StableDiffusionLoraLoaderMixin\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import cast_training_params, compute_snr\nfrom diffusers.utils import (\n    check_min_version,\n    convert_state_dict_to_diffusers,\n    convert_unet_state_dict_to_peft,\n    is_wandb_available,\n)\nfrom diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\nfrom diffusers.utils.import_utils import is_torch_npu_available, is_xformers_available\nfrom diffusers.utils.torch_utils import is_compiled_module\n\nif is_wandb_available():\n    import wandb\n\ncheck_min_version(\"0.18.0\")\n\nlogger = get_logger(__name__)\nif is_torch_npu_available():\n    torch.npu.config.allow_internal_format = False\n\n\ndef save_model_card(\n    repo_id: str,\n    images: list = None,\n    base_model: str = None,\n    dataset_name: str = None,\n    train_text_encoder: bool = False,\n    repo_folder: str = None,\n    vae_path: str = None,\n):\n    img_str = \"\"\n    if images is not None:\n        for i, image in enumerate(images):\n            image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n            img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n\n    model_description = f\"\"\"\n# LoRA text2image fine-tuning - {repo_id}\n\nThese are LoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset.\\n\n{img_str}\n\nLoRA for the text encoder was enabled: {train_text_encoder}.\n\nSpecial VAE used for training: {vae_path}.\n\"\"\"\n    model_card = load_or_create_model_card(\n        repo_id_or_path=repo_id,\n        from_training=True,\n        license=\"creativeml-openrail-m\",\n        base_model=base_model,\n        model_description=model_description,\n        inference=True,\n    )\n\n    tags = [\"stable-diffusion\", \"stable-diffusion-diffusers\", \"text-to-image\", \"diffusers\", \"diffusers-training\", \"lora\"]\n    model_card = populate_model_card(model_card, tags=tags)\n    model_card.save(os.path.join(repo_folder, \"README.md\"))\n\n\ndef log_validation(\n    pipeline,\n    args,\n    accelerator,\n    epoch,\n    is_final_validation=False,\n):\n    logger.info(\n        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n        f\" {args.validation_prompt}.\"\n    )\n    pipeline = pipeline.to(accelerator.device)\n    pipeline.set_progress_bar_config(disable=True)\n\n    # run inference\n    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None\n    pipeline_args = {\"prompt\": args.validation_prompt, \"num_inference_steps\": 30, \"guidance_scale\": 7.5}\n    if torch.backends.mps.is_available():\n        autocast_ctx = nullcontext()\n    else:\n        autocast_ctx = torch.autocast(accelerator.device.type)\n\n    with autocast_ctx:\n        images = [pipeline(**pipeline_args, generator=generator).images[0] for _ in range(args.num_validation_images)]\n\n    for tracker in accelerator.trackers:\n        phase_name = \"test\" if is_final_validation else \"validation\"\n        if tracker.name == \"tensorboard\":\n            np_images = np.stack([np.asarray(img) for img in images])\n            tracker.writer.add_images(phase_name, np_images, epoch, dataformats=\"NHWC\")\n        if tracker.name == \"wandb\":\n            tracker.log({phase_name: [wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)]})\n    return images\n\n\ndef parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script for Stable Diffusion 1.5 with LoRA.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--pretrained_vae_model_name_or_path\",\n        type=str,\n        default=None,\n        help=\"Path to pretrained VAE model, if desired.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=\"The name of the Dataset (from HuggingFace hub) or a path to a local dataset.\",\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, if needed.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=\"A folder containing the training data with `metadata.jsonl` for captions.\",\n    )\n    parser.add_argument(\n        \"--image_column\",\n        type=str,\n        default=\"image\",\n        help=\"The column of the dataset containing an image.\",\n    )\n    parser.add_argument(\n        \"--caption_column\",\n        type=str,\n        default=\"text\",\n        help=\"The column of the dataset containing a caption.\",\n    )\n    parser.add_argument(\n        \"--validation_prompt\",\n        type=str,\n        default=None,\n        help=\"A prompt for validation.\",\n    )\n    parser.add_argument(\n        \"--num_validation_images\",\n        type=int,\n        default=4,\n        help=\"Number of images generated during validation.\",\n    )\n    parser.add_argument(\n        \"--validation_epochs\",\n        type=int,\n        default=1,\n        help=\"Run validation every X epochs.\",\n    )\n    parser.add_argument(\n        \"--max_train_samples\",\n        type=int,\n        default=None,\n        help=\"For debugging or smaller training, limit the number of training samples.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"sd-model-finetuned-lora\",\n        help=\"The output directory for model predictions and checkpoints.\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        default=None,\n        help=\"Directory to store downloaded models and datasets.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=\"The resolution for input images, images will be resized to this resolution.\",\n    )\n    parser.add_argument(\n        \"--center_crop\",\n        default=False,\n        action=\"store_true\",\n        help=\"Whether to center crop input images.\",\n    )\n    parser.add_argument(\n        \"--random_flip\",\n        action=\"store_true\",\n        help=\"Whether to randomly flip images horizontally.\",\n    )\n    parser.add_argument(\n        \"--train_text_encoder\",\n        action=\"store_true\",\n        help=\"Whether to train the text encoder.\",\n    )\n    parser.add_argument(\n        \"--train_batch_size\",\n        type=int,\n        default=4,\n        help=\"Batch size per device for the training dataloader.\",\n    )\n    parser.add_argument(\"--num_train_epochs\", type=int, default=10)\n    parser.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=None,\n        help=\"Total training steps. Overrides num_train_epochs if set.\",\n    )\n    parser.add_argument(\n        \"--checkpointing_steps\",\n        type=int,\n        default=500,\n        help=\"Save a checkpoint of the training state every X steps.\",\n    )\n    parser.add_argument(\n        \"--checkpoints_total_limit\",\n        type=int,\n        default=None,\n        help=\"Max number of checkpoints to store.\",\n    )\n    parser.add_argument(\n        \"--resume_from_checkpoint\",\n        type=str,\n        default=None,\n        help='Resume from a previous checkpoint, use \"latest\" to resume from the most recent checkpoint.',\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of steps to accumulate before a backward pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Use gradient checkpointing to reduce memory usage.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate.\",\n    )\n    parser.add_argument(\n        \"--scale_lr\",\n        action=\"store_true\",\n        default=False,\n        help=\"Scale the learning rate by number of GPUs, steps, etc.\",\n    )\n    parser.add_argument(\n        \"--lr_scheduler\",\n        type=str,\n        default=\"constant\",\n        help='The scheduler type [\"linear\", \"cosine\", ...] or \"constant\".',\n    )\n    parser.add_argument(\n        \"--lr_warmup_steps\",\n        type=int,\n        default=500,\n        help=\"Number of steps for LR warmup.\",\n    )\n    parser.add_argument(\n        \"--snr_gamma\",\n        type=float,\n        default=None,\n        help=\"SNR weighting gamma. See related research for details.\",\n    )\n    parser.add_argument(\n        \"--allow_tf32\",\n        action=\"store_true\",\n        help=\"Allow TF32 on Ampere GPUs.\",\n    )\n    parser.add_argument(\n        \"--dataloader_num_workers\",\n        type=int,\n        default=0,\n        help=\"Number of subprocesses for data loading.\",\n    )\n    parser.add_argument(\n        \"--use_8bit_adam\",\n        action=\"store_true\",\n        help=\"Use 8-bit Adam for lower memory usage.\",\n    )\n    parser.add_argument(\"--adam_beta1\", type=float, default=0.9)\n    parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2)\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float)\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n    parser.add_argument(\"--hub_token\", type=str, default=None)\n    parser.add_argument(\n        \"--prediction_type\",\n        type=str,\n        default=None,\n        help=\"Prediction type for the scheduler: 'epsilon' or 'v_prediction'.\",\n    )\n    parser.add_argument(\"--hub_model_id\", type=str, default=None)\n    parser.add_argument(\n        \"--logging_dir\",\n        type=str,\n        default=\"logs\",\n    )\n    parser.add_argument(\n        \"--report_to\",\n        type=str,\n        default=\"tensorboard\",\n        help='Reporting integration: \"tensorboard\", \"wandb\", \"comet_ml\", or \"all\".',\n    )\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=None,\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=\"Use mixed precision training.\",\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\n        \"--enable_xformers_memory_efficient_attention\",\n        action=\"store_true\",\n        help=\"Use xformers memory-efficient attention.\",\n    )\n    parser.add_argument(\"--noise_offset\", type=float, default=0)\n    parser.add_argument(\n        \"--rank\",\n        type=int,\n        default=4,\n        help=\"The LoRA rank.\",\n    )\n    parser.add_argument(\n        \"--debug_loss\",\n        action=\"store_true\",\n        help=\"Log debug loss per image if filenames are available.\",\n    )\n\n    if input_args is not None:\n        args = parser.parse_args(input_args)\n    else:\n        args = parser.parse_args()\n\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    if args.dataset_name is None and args.train_data_dir is None:\n        raise ValueError(\"Need either a dataset name or a training folder.\")\n\n    return args\n\nDATASET_NAME_MAPPING = {\n    \"lambdalabs/naruto-blip-captions\": (\"image\", \"text\"),\n}\n\n\ndef tokenize_prompt(tokenizer, prompt):\n    text_inputs = tokenizer(\n        prompt,\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return text_inputs.input_ids\n\n\ndef main(args):\n    if args.report_to == \"wandb\" and args.hub_token is not None:\n        raise ValueError(\n            \"You cannot use both --report_to=wandb and --hub_token due to security risks.\"\n        )\n\n    logging_dir = Path(args.output_dir, args.logging_dir)\n\n    if torch.backends.mps.is_available() and args.mixed_precision == \"bf16\":\n        raise ValueError(\"Mixed precision bfloat16 not supported on MPS, use fp16 or no.\")\n\n    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n        log_with=args.report_to,\n        project_config=accelerator_project_config,\n        kwargs_handlers=[kwargs],\n    )\n\n    # Logging setup\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # Set seed\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle repository creation\n    if accelerator.is_main_process:\n        if args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n        if args.push_to_hub:\n            repo_id = create_repo(\n                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n            ).repo_id\n    else:\n        repo_id = None\n\n    # Load tokenizer and model components\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"tokenizer\",\n        revision=None,\n        use_fast=False,\n    )\n\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n\n    text_encoder = CLIPTextModel.from_pretrained(\n        args.pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=None,\n    )\n\n    vae_path = args.pretrained_model_name_or_path if args.pretrained_vae_model_name_or_path is None else args.pretrained_vae_model_name_or_path\n    vae = AutoencoderKL.from_pretrained(\n        vae_path,\n        subfolder=\"vae\" if args.pretrained_vae_model_name_or_path is None else None,\n        revision=None,\n    )\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=None\n    )\n\n    # Freeze base model\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    unet.requires_grad_(False)\n\n    # Cast non-trainable weights to half precision (if fp16 or bf16)\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    unet.to(accelerator.device, dtype=weight_dtype)\n    if args.pretrained_vae_model_name_or_path is None:\n        vae.to(accelerator.device, dtype=torch.float32)\n    else:\n        vae.to(accelerator.device, dtype=weight_dtype)\n    text_encoder.to(accelerator.device, dtype=weight_dtype)\n\n    if args.enable_xformers_memory_efficient_attention:\n        if is_xformers_available():\n            unet.enable_xformers_memory_efficient_attention()\n        else:\n            raise ValueError(\"xformers not available. Install with `pip install xformers`.\")\n\n    # Add LoRA\n    unet_lora_config = LoraConfig(\n        r=args.rank,\n        lora_alpha=args.rank,\n        init_lora_weights=\"gaussian\",\n        target_modules=[\"query\", \"key\", \"value\", \"proj_out\"],\n    )\n    unet.add_adapter(unet_lora_config)\n\n    if args.train_text_encoder:\n        # Text encoder LoRA\n        text_lora_config = LoraConfig(\n            r=args.rank,\n            lora_alpha=args.rank,\n            init_lora_weights=\"gaussian\",\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n        )\n        text_encoder.add_adapter(text_lora_config)\n\n    def unwrap_model(model):\n        model = accelerator.unwrap_model(model)\n        model = model._orig_mod if is_compiled_module(model) else model\n        return model\n\n    def save_model_hook(models, weights, output_dir):\n        if accelerator.is_main_process:\n            unet_lora_layers_to_save = None\n            text_encoder_lora_layers_to_save = None\n\n            for model in models:\n                if isinstance(unwrap_model(model), type(unwrap_model(unet))):\n                    unet_lora_layers_to_save = convert_state_dict_to_diffusers(get_peft_model_state_dict(model))\n                elif isinstance(unwrap_model(model), type(unwrap_model(text_encoder))):\n                    text_encoder_lora_layers_to_save = convert_state_dict_to_diffusers(get_peft_model_state_dict(model))\n                else:\n                    raise ValueError(\"Unexpected model in save hook.\")\n\n                if weights:\n                    weights.pop()\n\n            StableDiffusionPipeline.save_lora_weights(\n                output_dir,\n                unet_lora_layers=unet_lora_layers_to_save,\n                text_encoder_lora_layers=text_encoder_lora_layers_to_save,\n            )\n\n    def load_model_hook(models, input_dir):\n        unet_ = None\n        text_encoder_ = None\n\n        while len(models) > 0:\n            model = models.pop()\n            if isinstance(model, type(unwrap_model(unet))):\n                unet_ = model\n            elif isinstance(model, type(unwrap_model(text_encoder))):\n                text_encoder_ = model\n            else:\n                raise ValueError(\"Unexpected model in load hook.\")\n\n        lora_state_dict, _ = StableDiffusionLoraLoaderMixin.lora_state_dict(input_dir)\n        unet_state_dict = {k.replace(\"unet.\", \"\"): v for k, v in lora_state_dict.items() if k.startswith(\"unet.\")}\n        unet_state_dict = convert_unet_state_dict_to_peft(unet_state_dict)\n        set_peft_model_state_dict(unet_, unet_state_dict, adapter_name=\"default\")\n\n        if args.train_text_encoder:\n            # load into text encoder\n            text_encoder_keys = {k: v for k, v in lora_state_dict.items() if k.startswith(\"text_encoder.\")}\n            text_encoder_.load_state_dict(text_encoder_keys, strict=False)\n\n        if args.mixed_precision == \"fp16\":\n            models = [unet_]\n            if args.train_text_encoder:\n                models.append(text_encoder_)\n            cast_training_params(models, dtype=torch.float32)\n\n    accelerator.register_save_state_pre_hook(save_model_hook)\n    accelerator.register_load_state_pre_hook(load_model_hook)\n\n    if args.gradient_checkpointing:\n        unet.enable_gradient_checkpointing()\n        if args.train_text_encoder:\n            text_encoder.gradient_checkpointing_enable()\n\n    if args.allow_tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    if args.scale_lr:\n        args.learning_rate = (\n            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n        )\n\n    if args.mixed_precision == \"fp16\":\n        models = [unet]\n        if args.train_text_encoder:\n            models.append(text_encoder)\n        cast_training_params(models, dtype=torch.float32)\n\n    # Optimizer\n    if args.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\"Install bitsandbytes for 8-bit Adam: `pip install bitsandbytes`.\")\n        optimizer_class = bnb.optim.AdamW8bit\n    else:\n        optimizer_class = torch.optim.AdamW\n\n    params_to_optimize = list(filter(lambda p: p.requires_grad, unet.parameters()))\n    if args.train_text_encoder:\n        params_to_optimize += list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n\n    optimizer = optimizer_class(\n        params_to_optimize,\n        lr=args.learning_rate,\n        betas=(args.adam_beta1, args.adam_beta2),\n        weight_decay=args.adam_weight_decay,\n        eps=args.adam_epsilon,\n    )\n\n    # Load dataset\n    if args.dataset_name is not None:\n        dataset = load_dataset(\n            args.dataset_name, args.dataset_config_name, cache_dir=args.cache_dir, data_dir=args.train_data_dir\n        )\n    else:\n        data_files = {}\n        if args.train_data_dir is not None:\n            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n        dataset = load_dataset(\n            \"imagefolder\",\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n\n    column_names = dataset[\"train\"].column_names\n    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n    if args.image_column is None:\n        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n    else:\n        image_column = args.image_column\n    if args.caption_column is None:\n        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n    else:\n        caption_column = args.caption_column\n\n    def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\"Caption column should contain strings or list of strings.\")\n        tokens = tokenize_prompt(tokenizer, captions)\n        return tokens\n\n    train_transforms = transforms.Compose(\n        [\n            transforms.Resize((args.resolution, args.resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.Lambda(lambda x: x),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ]\n    )\n\n    def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        processed_images = [train_transforms(image) for image in images]\n        examples[\"pixel_values\"] = processed_images\n        input_ids = tokenize_captions(examples)\n        examples[\"input_ids\"] = input_ids\n        if args.debug_loss:\n            fnames = [os.path.basename(image.filename) for image in examples[image_column] if image.filename]\n            if fnames:\n                examples[\"filenames\"] = fnames\n        return examples\n\n    with accelerator.main_process_first():\n        if args.max_train_samples is not None:\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train, output_all_columns=True)\n\n    def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n        result = {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": input_ids,\n        }\n        filenames = [example[\"filenames\"] for example in examples if \"filenames\" in example]\n        if filenames:\n            result[\"filenames\"] = filenames\n        return result\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset,\n        shuffle=True,\n        collate_fn=collate_fn,\n        batch_size=args.train_batch_size,\n        num_workers=args.dataloader_num_workers,\n    )\n\n    # Scheduler\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n\n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n\n    if args.train_text_encoder:\n        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n        )\n    else:\n        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n            unet, optimizer, train_dataloader, lr_scheduler\n        )\n\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    if accelerator.is_main_process:\n        accelerator.init_trackers(\"sd-lora-fine-tune\", config=vars(args))\n\n    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(f\"  Total train batch size = {total_batch_size}\")\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    global_step = 0\n    first_epoch = 0\n\n    # Resume\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint != \"latest\":\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [d for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint\")]\n            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1])) if len(dirs) > 0 else []\n            path = dirs[-1] if len(dirs) > 0 else None\n\n        if path is None:\n            accelerator.print(f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting fresh.\")\n            args.resume_from_checkpoint = None\n            initial_global_step = 0\n        else:\n            accelerator.print(f\"Resuming from checkpoint {path}\")\n            accelerator.load_state(os.path.join(args.output_dir, path))\n            global_step = int(path.split(\"-\")[1])\n            initial_global_step = global_step\n            first_epoch = global_step // num_update_steps_per_epoch\n    else:\n        initial_global_step = 0\n\n    progress_bar = tqdm(\n        range(global_step, args.max_train_steps),\n        desc=\"Steps\",\n        disable=not accelerator.is_local_main_process,\n    )\n\n    # Training loop\n    for epoch in range(first_epoch, args.num_train_epochs):\n        unet.train()\n        if args.train_text_encoder:\n            text_encoder.train()\n        train_loss = 0.0\n\n        for step, batch in enumerate(train_dataloader):\n            with accelerator.accumulate(unet):\n                pixel_values = batch[\"pixel_values\"]\n                model_input = vae.encode(pixel_values).latent_dist.sample()\n                model_input = model_input * vae.config.scaling_factor\n                if args.pretrained_vae_model_name_or_path is None:\n                    model_input = model_input.to(weight_dtype)\n\n                noise = torch.randn_like(model_input)\n                if args.noise_offset:\n                    noise += args.noise_offset * torch.randn((model_input.shape[0], model_input.shape[1], 1, 1), device=model_input.device)\n\n                bsz = model_input.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=model_input.device).long()\n                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n\n                # Encode prompts\n                text_input_ids = batch[\"input_ids\"].to(accelerator.device)\n                text_encoder_out = text_encoder(text_input_ids)\n                text_embeddings = text_encoder_out[0]\n\n                model_pred = unet(noisy_model_input, timesteps, encoder_hidden_states=text_embeddings).sample\n\n                if args.prediction_type is not None:\n                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)\n\n                if noise_scheduler.config.prediction_type == \"epsilon\":\n                    target = noise\n                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n                else:\n                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n                if args.snr_gamma is None:\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n                else:\n                    snr = compute_snr(noise_scheduler, timesteps)\n                    mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n                    if noise_scheduler.config.prediction_type == \"epsilon\":\n                        mse_loss_weights = mse_loss_weights / snr\n                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                        mse_loss_weights = mse_loss_weights / (snr + 1)\n                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n                    loss = loss.mean()\n\n                if args.debug_loss and \"filenames\" in batch:\n                    for fname in batch[\"filenames\"]:\n                        accelerator.log({\"loss_for_\" + fname: loss}, step=global_step)\n\n                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n\n                accelerator.backward(loss)\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(params_to_optimize, args.max_grad_norm)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                global_step += 1\n                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n                train_loss = 0.0\n\n                # Checkpointing\n                if accelerator.is_main_process and global_step % args.checkpointing_steps == 0:\n                    if args.checkpoints_total_limit is not None:\n                        checkpoints = [d for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint\")]\n                        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n                        if len(checkpoints) >= args.checkpoints_total_limit:\n                            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n                            removing_checkpoints = checkpoints[0:num_to_remove]\n                            for removing_checkpoint in removing_checkpoints:\n                                shutil.rmtree(os.path.join(args.output_dir, removing_checkpoint))\n                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n                    accelerator.save_state(save_path)\n                    logger.info(f\"Saved state to {save_path}\")\n\n            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n            if global_step >= args.max_train_steps:\n                break\n\n        if accelerator.is_main_process and args.validation_prompt is not None and epoch % args.validation_epochs == 0:\n            pipeline = StableDiffusionPipeline.from_pretrained(\n                args.pretrained_model_name_or_path,\n                vae=vae,\n                text_encoder=unwrap_model(text_encoder),\n                unet=unwrap_model(unet),\n                torch_dtype=weight_dtype,\n            )\n            pipeline.load_lora_weights(args.output_dir)\n            images = log_validation(pipeline, args, accelerator, epoch)\n            del pipeline\n            torch.cuda.empty_cache()\n\n    # Save final LoRA weights\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        unet = unwrap_model(unet)\n        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unet))\n\n        if args.train_text_encoder:\n            text_encoder = unwrap_model(text_encoder)\n            text_encoder_lora_layers = convert_state_dict_to_diffusers(get_peft_model_state_dict(text_encoder))\n        else:\n            text_encoder_lora_layers = None\n\n        StableDiffusionPipeline.save_lora_weights(\n            save_directory=args.output_dir,\n            unet_lora_layers=unet_lora_state_dict,\n            text_encoder_lora_layers=text_encoder_lora_layers,\n        )\n\n        del unet\n        del text_encoder_lora_layers\n        torch.cuda.empty_cache()\n\n        # Final inference\n        if args.mixed_precision == \"fp16\":\n            vae.to(weight_dtype)\n\n        pipeline = StableDiffusionPipeline.from_pretrained(\n            args.pretrained_model_name_or_path,\n            vae=vae,\n            torch_dtype=weight_dtype,\n        )\n        pipeline.load_lora_weights(args.output_dir)\n\n        if args.validation_prompt and args.num_validation_images > 0:\n            images = log_validation(pipeline, args, accelerator, epoch, is_final_validation=True)\n\n        if args.push_to_hub:\n            save_model_card(\n                repo_id,\n                images=images if args.validation_prompt else None,\n                base_model=args.pretrained_model_name_or_path,\n                dataset_name=args.dataset_name,\n                train_text_encoder=args.train_text_encoder,\n                repo_folder=args.output_dir,\n                vae_path=args.pretrained_vae_model_name_or_path,\n            )\n            upload_folder(\n                repo_id=repo_id,\n                folder_path=args.output_dir,\n                commit_message=\"End of training\",\n                ignore_patterns=[\"step_*\", \"epoch_*\"],\n            )\n\n    accelerator.end_training()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args)\n","metadata":{"id":"nToQLotywNXy","jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets transformers peft","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEVY1kRj5242","outputId":"c5fc46b4-57fb-4d27-88fa-9dc1183d69d8","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:30:44.032260Z","iopub.execute_input":"2024-12-13T05:30:44.033061Z","iopub.status.idle":"2024-12-13T05:30:52.501213Z","shell.execute_reply.started":"2024-12-13T05:30:44.033025Z","shell.execute_reply":"2024-12-13T05:30:52.500026Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/huggingface/diffusers.git\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnsheZdMB9IH","outputId":"a3e70f12-a3e8-4355-9b15-fab26f77d3a1","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:23:46.506183Z","iopub.execute_input":"2024-12-13T05:23:46.506532Z","iopub.status.idle":"2024-12-13T05:24:10.925851Z","shell.execute_reply.started":"2024-12-13T05:23:46.506501Z","shell.execute_reply":"2024-12-13T05:24:10.924750Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/diffusers.git\n  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-8ghdzr0o\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-8ghdzr0o\n  Resolved https://github.com/huggingface/diffusers.git to commit ec9bfa9e148b7764137dd92247ce859d915abcb0\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: importlib_metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (0.26.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib_metadata->diffusers==0.32.0.dev0) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (2024.6.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (3.1.2)\nBuilding wheels for collected packages: diffusers\n  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.32.0.dev0-py3-none-any.whl size=3120417 sha256=e8af64772babb11aad72efa9934f34c848d14cf78ca04fc4aa658206d8f80101\n  Stored in directory: /tmp/pip-ephem-wheel-cache-18s9xco8/wheels/4d/b7/a8/6f9549ceec5daad78675b857ac57d697c387062506520a7b50\nSuccessfully built diffusers\nInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.32.0.dev0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!accelerate launch /content/train_lora_sd15.py \\\n  --pretrained_model_name_or_path playgroundai/playground-v2.5-1024px-aesthetic \\\n  --train_data_dir /content/new \\\n  --caption_column text \\\n  --resolution 512 \\\n  --learning_rate 1e-4 \\\n  --train_batch_size 4 \\\n  --num_train_epochs 10 \\\n  --output_dir ./lora-output\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aa4QedyOxIS0","outputId":"f27ea22c-9ae6-4b66-913a-6c13e7578364","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"stream","name":"stdout","text":["The following values were not passed to `accelerate launch` and had defaults used instead:\n","\t`--num_processes` was set to a value of `1`\n","\t`--num_machines` was set to a value of `1`\n","\t`--mixed_precision` was set to a value of `'no'`\n","\t`--dynamo_backend` was set to a value of `'no'`\n","To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n","2024-12-12 08:32:24.692028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-12 08:32:24.711736: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-12 08:32:24.717645: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-12 08:32:24.732544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-12-12 08:32:26.270470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","12/12/2024 08:32:28 - INFO - __main__ - Distributed environment: NO\n","Num processes: 1\n","Process index: 0\n","Local process index: 0\n","Device: cuda\n","\n","Mixed precision type: no\n","\n","{'steps_offset', 'timestep_spacing', 'trained_betas', 'beta_start', 'beta_schedule', 'rescale_betas_zero_snr', 'variance_type', 'clip_sample_range', 'beta_end', 'clip_sample'} was not found in config. Values will be initialized to default values.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Resolving data files: 100% 1001/1001 [00:00<00:00, 57470.38it/s]\n","Downloading data: 100% 1001/1001 [00:00<00:00, 205707.90files/s]\n","Generating train split: 1000 examples [00:00, 5056.40 examples/s]\n","12/12/2024 08:33:31 - INFO - __main__ - ***** Running training *****\n","12/12/2024 08:33:31 - INFO - __main__ -   Num examples = 1000\n","12/12/2024 08:33:31 - INFO - __main__ -   Num Epochs = 10\n","12/12/2024 08:33:31 - INFO - __main__ -   Instantaneous batch size per device = 4\n","12/12/2024 08:33:31 - INFO - __main__ -   Total train batch size = 4\n","12/12/2024 08:33:31 - INFO - __main__ -   Gradient Accumulation steps = 1\n","12/12/2024 08:33:31 - INFO - __main__ -   Total optimization steps = 2500\n","Steps:   0% 0/2500 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/train_lora_sd15.py\", line 951, in <module>\n","    main(args)\n","  File \"/content/train_lora_sd15.py\", line 815, in main\n","    model_pred = unet(noisy_model_input, timesteps, encoder_hidden_states=text_embeddings).sample\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py\", line 1152, in forward\n","    aug_emb = self.get_aug_embed(\n","  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py\", line 970, in get_aug_embed\n","    if \"text_embeds\" not in added_cond_kwargs:\n","TypeError: argument of type 'NoneType' is not iterable\n","Steps:   0% 0/2500 [00:03<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1168, in launch_command\n","    simple_launcher(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\n","    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n","subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/train_lora_sd15.py', '--pretrained_model_name_or_path', 'playgroundai/playground-v2.5-1024px-aesthetic', '--train_data_dir', '/content/new', '--caption_column', 'text', '--resolution', '512', '--learning_rate', '1e-4', '--train_batch_size', '4', '--num_train_epochs', '10', '--output_dir', './lora-output']' returned non-zero exit status 1.\n"]}],"execution_count":10},{"cell_type":"code","source":"!unzip /content/new.zip -d /content/\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQDGgyY21X_x","outputId":"b0109281-0117-4ae2-87ef-fb2f1504b19c","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/new.zip\n","  inflating: /content/new/Hand_0000002.jpg  \n","  inflating: /content/new/Hand_0000003.jpg  \n","  inflating: /content/new/Hand_0000004.jpg  \n","  inflating: /content/new/Hand_0000005.jpg  \n","  inflating: /content/new/Hand_0000006.jpg  \n","  inflating: /content/new/Hand_0000007.jpg  \n","  inflating: /content/new/Hand_0000008.jpg  \n","  inflating: /content/new/Hand_0000009.jpg  \n","  inflating: /content/new/Hand_0000010.jpg  \n","  inflating: /content/new/Hand_0000011.jpg  \n","  inflating: /content/new/Hand_0000012.jpg  \n","  inflating: /content/new/Hand_0000013.jpg  \n","  inflating: /content/new/Hand_0000014.jpg  \n","  inflating: /content/new/Hand_0000015.jpg  \n","  inflating: /content/new/Hand_0000016.jpg  \n","  inflating: /content/new/Hand_0000020.jpg  \n","  inflating: /content/new/Hand_0000021.jpg  \n","  inflating: /content/new/Hand_0000022.jpg  \n","  inflating: /content/new/Hand_0000023.jpg  \n","  inflating: /content/new/Hand_0000024.jpg  \n","  inflating: /content/new/Hand_0000025.jpg  \n","  inflating: /content/new/Hand_0000026.jpg  \n","  inflating: /content/new/Hand_0000027.jpg  \n","  inflating: /content/new/Hand_0000028.jpg  \n","  inflating: /content/new/Hand_0000029.jpg  \n","  inflating: /content/new/Hand_0000030.jpg  \n","  inflating: /content/new/Hand_0000031.jpg  \n","  inflating: /content/new/Hand_0000032.jpg  \n","  inflating: /content/new/Hand_0000033.jpg  \n","  inflating: /content/new/Hand_0000034.jpg  \n","  inflating: /content/new/Hand_0000038.jpg  \n","  inflating: /content/new/Hand_0000039.jpg  \n","  inflating: /content/new/Hand_0000040.jpg  \n","  inflating: /content/new/Hand_0000041.jpg  \n","  inflating: /content/new/Hand_0000042.jpg  \n","  inflating: /content/new/Hand_0000043.jpg  \n","  inflating: /content/new/Hand_0000044.jpg  \n","  inflating: /content/new/Hand_0000045.jpg  \n","  inflating: /content/new/Hand_0000046.jpg  \n","  inflating: /content/new/Hand_0000047.jpg  \n","  inflating: /content/new/Hand_0000048.jpg  \n","  inflating: /content/new/Hand_0000049.jpg  \n","  inflating: /content/new/Hand_0000050.jpg  \n","  inflating: /content/new/Hand_0000051.jpg  \n","  inflating: /content/new/Hand_0000052.jpg  \n","  inflating: /content/new/Hand_0000053.jpg  \n","  inflating: /content/new/Hand_0000054.jpg  \n","  inflating: /content/new/Hand_0000055.jpg  \n","  inflating: /content/new/Hand_0000056.jpg  \n","  inflating: /content/new/Hand_0000057.jpg  \n","  inflating: /content/new/Hand_0000058.jpg  \n","  inflating: /content/new/Hand_0000059.jpg  \n","  inflating: /content/new/Hand_0000060.jpg  \n","  inflating: /content/new/Hand_0000061.jpg  \n","  inflating: /content/new/Hand_0000062.jpg  \n","  inflating: /content/new/Hand_0000063.jpg  \n","  inflating: /content/new/Hand_0000064.jpg  \n","  inflating: /content/new/Hand_0000065.jpg  \n","  inflating: /content/new/Hand_0000066.jpg  \n","  inflating: /content/new/Hand_0000067.jpg  \n","  inflating: /content/new/Hand_0000068.jpg  \n","  inflating: /content/new/Hand_0000069.jpg  \n","  inflating: /content/new/Hand_0000070.jpg  \n","  inflating: /content/new/Hand_0000071.jpg  \n","  inflating: /content/new/Hand_0000072.jpg  \n","  inflating: /content/new/Hand_0000073.jpg  \n","  inflating: /content/new/Hand_0000074.jpg  \n","  inflating: /content/new/Hand_0000075.jpg  \n","  inflating: /content/new/Hand_0000076.jpg  \n","  inflating: /content/new/Hand_0000077.jpg  \n","  inflating: /content/new/Hand_0000078.jpg  \n","  inflating: /content/new/Hand_0000079.jpg  \n","  inflating: /content/new/Hand_0000080.jpg  \n","  inflating: /content/new/Hand_0000081.jpg  \n","  inflating: /content/new/Hand_0000082.jpg  \n","  inflating: /content/new/Hand_0000083.jpg  \n","  inflating: /content/new/Hand_0000084.jpg  \n","  inflating: /content/new/Hand_0000085.jpg  \n","  inflating: /content/new/Hand_0000086.jpg  \n","  inflating: /content/new/Hand_0000087.jpg  \n","  inflating: /content/new/Hand_0000088.jpg  \n","  inflating: /content/new/Hand_0000089.jpg  \n","  inflating: /content/new/Hand_0000090.jpg  \n","  inflating: /content/new/Hand_0000091.jpg  \n","  inflating: /content/new/Hand_0000092.jpg  \n","  inflating: /content/new/Hand_0000093.jpg  \n","  inflating: /content/new/Hand_0000094.jpg  \n","  inflating: /content/new/Hand_0000095.jpg  \n","  inflating: /content/new/Hand_0000096.jpg  \n","  inflating: /content/new/Hand_0000097.jpg  \n","  inflating: /content/new/Hand_0000100.jpg  \n","  inflating: /content/new/Hand_0000101.jpg  \n","  inflating: /content/new/Hand_0000102.jpg  \n","  inflating: /content/new/Hand_0000103.jpg  \n","  inflating: /content/new/Hand_0000104.jpg  \n","  inflating: /content/new/Hand_0000105.jpg  \n","  inflating: /content/new/Hand_0000106.jpg  \n","  inflating: /content/new/Hand_0000107.jpg  \n","  inflating: /content/new/Hand_0000108.jpg  \n","  inflating: /content/new/Hand_0000109.jpg  \n","  inflating: /content/new/Hand_0000111.jpg  \n","  inflating: /content/new/Hand_0000112.jpg  \n","  inflating: /content/new/Hand_0000113.jpg  \n","  inflating: /content/new/Hand_0000114.jpg  \n","  inflating: /content/new/Hand_0000115.jpg  \n","  inflating: /content/new/Hand_0000116.jpg  \n","  inflating: /content/new/Hand_0000117.jpg  \n","  inflating: /content/new/Hand_0000118.jpg  \n","  inflating: /content/new/Hand_0000122.jpg  \n","  inflating: /content/new/Hand_0000123.jpg  \n","  inflating: /content/new/Hand_0000124.jpg  \n","  inflating: /content/new/Hand_0000125.jpg  \n","  inflating: /content/new/Hand_0000132.jpg  \n","  inflating: /content/new/Hand_0000133.jpg  \n","  inflating: /content/new/Hand_0000134.jpg  \n","  inflating: /content/new/Hand_0000135.jpg  \n","  inflating: /content/new/Hand_0000140.jpg  \n","  inflating: /content/new/Hand_0000141.jpg  \n","  inflating: /content/new/Hand_0000142.jpg  \n","  inflating: /content/new/Hand_0000143.jpg  \n","  inflating: /content/new/Hand_0000144.jpg  \n","  inflating: /content/new/Hand_0000145.jpg  \n","  inflating: /content/new/Hand_0000146.jpg  \n","  inflating: /content/new/Hand_0000147.jpg  \n","  inflating: /content/new/Hand_0000148.jpg  \n","  inflating: /content/new/Hand_0000153.jpg  \n","  inflating: /content/new/Hand_0000156.jpg  \n","  inflating: /content/new/Hand_0000157.jpg  \n","  inflating: /content/new/Hand_0000158.jpg  \n","  inflating: /content/new/Hand_0000159.jpg  \n","  inflating: /content/new/Hand_0000160.jpg  \n","  inflating: /content/new/Hand_0000161.jpg  \n","  inflating: /content/new/Hand_0000162.jpg  \n","  inflating: /content/new/Hand_0000163.jpg  \n","  inflating: /content/new/Hand_0000164.jpg  \n","  inflating: /content/new/Hand_0000165.jpg  \n","  inflating: /content/new/Hand_0000166.jpg  \n","  inflating: /content/new/Hand_0000167.jpg  \n","  inflating: /content/new/Hand_0000169.jpg  \n","  inflating: /content/new/Hand_0000170.jpg  \n","  inflating: /content/new/Hand_0000171.jpg  \n","  inflating: /content/new/Hand_0000173.jpg  \n","  inflating: /content/new/Hand_0000174.jpg  \n","  inflating: /content/new/Hand_0000175.jpg  \n","  inflating: /content/new/Hand_0000176.jpg  \n","  inflating: /content/new/Hand_0000177.jpg  \n","  inflating: /content/new/Hand_0000178.jpg  \n","  inflating: /content/new/Hand_0000179.jpg  \n","  inflating: /content/new/Hand_0000180.jpg  \n","  inflating: /content/new/Hand_0000182.jpg  \n","  inflating: /content/new/Hand_0000183.jpg  \n","  inflating: /content/new/Hand_0000184.jpg  \n","  inflating: /content/new/Hand_0000185.jpg  \n","  inflating: /content/new/Hand_0000186.jpg  \n","  inflating: /content/new/Hand_0000187.jpg  \n","  inflating: /content/new/Hand_0000195.jpg  \n","  inflating: /content/new/Hand_0000196.jpg  \n","  inflating: /content/new/Hand_0000197.jpg  \n","  inflating: /content/new/Hand_0000198.jpg  \n","  inflating: /content/new/Hand_0000199.jpg  \n","  inflating: /content/new/Hand_0000200.jpg  \n","  inflating: /content/new/Hand_0000201.jpg  \n","  inflating: /content/new/Hand_0000202.jpg  \n","  inflating: /content/new/Hand_0000203.jpg  \n","  inflating: /content/new/Hand_0000204.jpg  \n","  inflating: /content/new/Hand_0000205.jpg  \n","  inflating: /content/new/Hand_0000206.jpg  \n","  inflating: /content/new/Hand_0000207.jpg  \n","  inflating: /content/new/Hand_0000208.jpg  \n","  inflating: /content/new/Hand_0000209.jpg  \n","  inflating: /content/new/Hand_0000210.jpg  \n","  inflating: /content/new/Hand_0000211.jpg  \n","  inflating: /content/new/Hand_0000212.jpg  \n","  inflating: /content/new/Hand_0000213.jpg  \n","  inflating: /content/new/Hand_0000214.jpg  \n","  inflating: /content/new/Hand_0000215.jpg  \n","  inflating: /content/new/Hand_0000216.jpg  \n","  inflating: /content/new/Hand_0000217.jpg  \n","  inflating: /content/new/Hand_0000218.jpg  \n","  inflating: /content/new/Hand_0000219.jpg  \n","  inflating: /content/new/Hand_0000220.jpg  \n","  inflating: /content/new/Hand_0000221.jpg  \n","  inflating: /content/new/Hand_0000223.jpg  \n","  inflating: /content/new/Hand_0000224.jpg  \n","  inflating: /content/new/Hand_0000225.jpg  \n","  inflating: /content/new/Hand_0000226.jpg  \n","  inflating: /content/new/Hand_0000227.jpg  \n","  inflating: /content/new/Hand_0000228.jpg  \n","  inflating: /content/new/Hand_0000229.jpg  \n","  inflating: /content/new/Hand_0000230.jpg  \n","  inflating: /content/new/Hand_0000231.jpg  \n","  inflating: /content/new/Hand_0000232.jpg  \n","  inflating: /content/new/Hand_0000233.jpg  \n","  inflating: /content/new/Hand_0000234.jpg  \n","  inflating: /content/new/Hand_0000235.jpg  \n","  inflating: /content/new/Hand_0000236.jpg  \n","  inflating: /content/new/Hand_0000237.jpg  \n","  inflating: /content/new/Hand_0000238.jpg  \n","  inflating: /content/new/Hand_0000239.jpg  \n","  inflating: /content/new/Hand_0000240.jpg  \n","  inflating: /content/new/Hand_0000241.jpg  \n","  inflating: /content/new/Hand_0000242.jpg  \n","  inflating: /content/new/Hand_0000243.jpg  \n","  inflating: /content/new/Hand_0000244.jpg  \n","  inflating: /content/new/Hand_0000245.jpg  \n","  inflating: /content/new/Hand_0000246.jpg  \n","  inflating: /content/new/Hand_0000248.jpg  \n","  inflating: /content/new/Hand_0000249.jpg  \n","  inflating: /content/new/Hand_0000250.jpg  \n","  inflating: /content/new/Hand_0000251.jpg  \n","  inflating: /content/new/Hand_0000252.jpg  \n","  inflating: /content/new/Hand_0000253.jpg  \n","  inflating: /content/new/Hand_0000254.jpg  \n","  inflating: /content/new/Hand_0000255.jpg  \n","  inflating: /content/new/Hand_0000256.jpg  \n","  inflating: /content/new/Hand_0000257.jpg  \n","  inflating: /content/new/Hand_0000258.jpg  \n","  inflating: /content/new/Hand_0000259.jpg  \n","  inflating: /content/new/Hand_0000260.jpg  \n","  inflating: /content/new/Hand_0000261.jpg  \n","  inflating: /content/new/Hand_0000262.jpg  \n","  inflating: /content/new/Hand_0000263.jpg  \n","  inflating: /content/new/Hand_0000264.jpg  \n","  inflating: /content/new/Hand_0000265.jpg  \n","  inflating: /content/new/Hand_0000266.jpg  \n","  inflating: /content/new/Hand_0000267.jpg  \n","  inflating: /content/new/Hand_0000268.jpg  \n","  inflating: /content/new/Hand_0000269.jpg  \n","  inflating: /content/new/Hand_0000270.jpg  \n","  inflating: /content/new/Hand_0000271.jpg  \n","  inflating: /content/new/Hand_0000272.jpg  \n","  inflating: /content/new/Hand_0000273.jpg  \n","  inflating: /content/new/Hand_0000275.jpg  \n","  inflating: /content/new/Hand_0000276.jpg  \n","  inflating: /content/new/Hand_0000277.jpg  \n","  inflating: /content/new/Hand_0000278.jpg  \n","  inflating: /content/new/Hand_0000281.jpg  \n","  inflating: /content/new/Hand_0000282.jpg  \n","  inflating: /content/new/Hand_0000283.jpg  \n","  inflating: /content/new/Hand_0000284.jpg  \n","  inflating: /content/new/Hand_0000285.jpg  \n","  inflating: /content/new/Hand_0000286.jpg  \n","  inflating: /content/new/Hand_0000287.jpg  \n","  inflating: /content/new/Hand_0000288.jpg  \n","  inflating: /content/new/Hand_0000289.jpg  \n","  inflating: /content/new/Hand_0000290.jpg  \n","  inflating: /content/new/Hand_0000291.jpg  \n","  inflating: /content/new/Hand_0000292.jpg  \n","  inflating: /content/new/Hand_0000293.jpg  \n","  inflating: /content/new/Hand_0000295.jpg  \n","  inflating: /content/new/Hand_0000296.jpg  \n","  inflating: /content/new/Hand_0000297.jpg  \n","  inflating: /content/new/Hand_0000298.jpg  \n","  inflating: /content/new/Hand_0000299.jpg  \n","  inflating: /content/new/Hand_0000300.jpg  \n","  inflating: /content/new/Hand_0000302.jpg  \n","  inflating: /content/new/Hand_0000303.jpg  \n","  inflating: /content/new/Hand_0000304.jpg  \n","  inflating: /content/new/Hand_0000305.jpg  \n","  inflating: /content/new/Hand_0000306.jpg  \n","  inflating: /content/new/Hand_0000307.jpg  \n","  inflating: /content/new/Hand_0000308.jpg  \n","  inflating: /content/new/Hand_0000309.jpg  \n","  inflating: /content/new/Hand_0000310.jpg  \n","  inflating: /content/new/Hand_0000311.jpg  \n","  inflating: /content/new/Hand_0000312.jpg  \n","  inflating: /content/new/Hand_0000314.jpg  \n","  inflating: /content/new/Hand_0000315.jpg  \n","  inflating: /content/new/Hand_0000316.jpg  \n","  inflating: /content/new/Hand_0000317.jpg  \n","  inflating: /content/new/Hand_0000318.jpg  \n","  inflating: /content/new/Hand_0000319.jpg  \n","  inflating: /content/new/Hand_0000320.jpg  \n","  inflating: /content/new/Hand_0000321.jpg  \n","  inflating: /content/new/Hand_0000322.jpg  \n","  inflating: /content/new/Hand_0000323.jpg  \n","  inflating: /content/new/Hand_0000324.jpg  \n","  inflating: /content/new/Hand_0000325.jpg  \n","  inflating: /content/new/Hand_0000326.jpg  \n","  inflating: /content/new/Hand_0000327.jpg  \n","  inflating: /content/new/Hand_0000328.jpg  \n","  inflating: /content/new/Hand_0000329.jpg  \n","  inflating: /content/new/Hand_0000330.jpg  \n","  inflating: /content/new/Hand_0000331.jpg  \n","  inflating: /content/new/Hand_0000332.jpg  \n","  inflating: /content/new/Hand_0000333.jpg  \n","  inflating: /content/new/Hand_0000345.jpg  \n","  inflating: /content/new/Hand_0000346.jpg  \n","  inflating: /content/new/Hand_0000347.jpg  \n","  inflating: /content/new/Hand_0000348.jpg  \n","  inflating: /content/new/Hand_0000349.jpg  \n","  inflating: /content/new/Hand_0000350.jpg  \n","  inflating: /content/new/Hand_0000351.jpg  \n","  inflating: /content/new/Hand_0000372.jpg  \n","  inflating: /content/new/Hand_0000373.jpg  \n","  inflating: /content/new/Hand_0000374.jpg  \n","  inflating: /content/new/Hand_0000375.jpg  \n","  inflating: /content/new/Hand_0000376.jpg  \n","  inflating: /content/new/Hand_0000377.jpg  \n","  inflating: /content/new/Hand_0000378.jpg  \n","  inflating: /content/new/Hand_0000381.jpg  \n","  inflating: /content/new/Hand_0000382.jpg  \n","  inflating: /content/new/Hand_0000383.jpg  \n","  inflating: /content/new/Hand_0000384.jpg  \n","  inflating: /content/new/Hand_0000385.jpg  \n","  inflating: /content/new/Hand_0000386.jpg  \n","  inflating: /content/new/Hand_0000387.jpg  \n","  inflating: /content/new/Hand_0000388.jpg  \n","  inflating: /content/new/Hand_0000389.jpg  \n","  inflating: /content/new/Hand_0000390.jpg  \n","  inflating: /content/new/Hand_0000391.jpg  \n","  inflating: /content/new/Hand_0000392.jpg  \n","  inflating: /content/new/Hand_0000393.jpg  \n","  inflating: /content/new/Hand_0000394.jpg  \n","  inflating: /content/new/Hand_0000395.jpg  \n","  inflating: /content/new/Hand_0000396.jpg  \n","  inflating: /content/new/Hand_0000397.jpg  \n","  inflating: /content/new/Hand_0000398.jpg  \n","  inflating: /content/new/Hand_0000399.jpg  \n","  inflating: /content/new/Hand_0000400.jpg  \n","  inflating: /content/new/Hand_0000401.jpg  \n","  inflating: /content/new/Hand_0000402.jpg  \n","  inflating: /content/new/Hand_0000403.jpg  \n","  inflating: /content/new/Hand_0000404.jpg  \n","  inflating: /content/new/Hand_0000405.jpg  \n","  inflating: /content/new/Hand_0000406.jpg  \n","  inflating: /content/new/Hand_0000407.jpg  \n","  inflating: /content/new/Hand_0000408.jpg  \n","  inflating: /content/new/Hand_0000410.jpg  \n","  inflating: /content/new/Hand_0000411.jpg  \n","  inflating: /content/new/Hand_0000412.jpg  \n","  inflating: /content/new/Hand_0000413.jpg  \n","  inflating: /content/new/Hand_0000414.jpg  \n","  inflating: /content/new/Hand_0000415.jpg  \n","  inflating: /content/new/Hand_0000416.jpg  \n","  inflating: /content/new/Hand_0000417.jpg  \n","  inflating: /content/new/Hand_0000418.jpg  \n","  inflating: /content/new/Hand_0000419.jpg  \n","  inflating: /content/new/Hand_0000420.jpg  \n","  inflating: /content/new/Hand_0000421.jpg  \n","  inflating: /content/new/Hand_0000422.jpg  \n","  inflating: /content/new/Hand_0000423.jpg  \n","  inflating: /content/new/Hand_0000424.jpg  \n","  inflating: /content/new/Hand_0000425.jpg  \n","  inflating: /content/new/Hand_0000426.jpg  \n","  inflating: /content/new/Hand_0000427.jpg  \n","  inflating: /content/new/Hand_0000428.jpg  \n","  inflating: /content/new/Hand_0000429.jpg  \n","  inflating: /content/new/Hand_0000430.jpg  \n","  inflating: /content/new/Hand_0000431.jpg  \n","  inflating: /content/new/Hand_0000432.jpg  \n","  inflating: /content/new/Hand_0000433.jpg  \n","  inflating: /content/new/Hand_0000437.jpg  \n","  inflating: /content/new/Hand_0000438.jpg  \n","  inflating: /content/new/Hand_0000439.jpg  \n","  inflating: /content/new/Hand_0000440.jpg  \n","  inflating: /content/new/Hand_0000441.jpg  \n","  inflating: /content/new/Hand_0000442.jpg  \n","  inflating: /content/new/Hand_0000443.jpg  \n","  inflating: /content/new/Hand_0000444.jpg  \n","  inflating: /content/new/Hand_0000445.jpg  \n","  inflating: /content/new/Hand_0000446.jpg  \n","  inflating: /content/new/Hand_0000447.jpg  \n","  inflating: /content/new/Hand_0000448.jpg  \n","  inflating: /content/new/Hand_0000449.jpg  \n","  inflating: /content/new/Hand_0000450.jpg  \n","  inflating: /content/new/Hand_0000451.jpg  \n","  inflating: /content/new/Hand_0000452.jpg  \n","  inflating: /content/new/Hand_0000454.jpg  \n","  inflating: /content/new/Hand_0000455.jpg  \n","  inflating: /content/new/Hand_0000456.jpg  \n","  inflating: /content/new/Hand_0000457.jpg  \n","  inflating: /content/new/Hand_0000458.jpg  \n","  inflating: /content/new/Hand_0000459.jpg  \n","  inflating: /content/new/Hand_0000460.jpg  \n","  inflating: /content/new/Hand_0000461.jpg  \n","  inflating: /content/new/Hand_0000462.jpg  \n","  inflating: /content/new/Hand_0000463.jpg  \n","  inflating: /content/new/Hand_0000465.jpg  \n","  inflating: /content/new/Hand_0000472.jpg  \n","  inflating: /content/new/Hand_0000473.jpg  \n","  inflating: /content/new/Hand_0000482.jpg  \n","  inflating: /content/new/Hand_0000483.jpg  \n","  inflating: /content/new/Hand_0000484.jpg  \n","  inflating: /content/new/Hand_0000485.jpg  \n","  inflating: /content/new/Hand_0000486.jpg  \n","  inflating: /content/new/Hand_0000488.jpg  \n","  inflating: /content/new/Hand_0000490.jpg  \n","  inflating: /content/new/Hand_0000491.jpg  \n","  inflating: /content/new/Hand_0000492.jpg  \n","  inflating: /content/new/Hand_0000493.jpg  \n","  inflating: /content/new/Hand_0000494.jpg  \n","  inflating: /content/new/Hand_0000495.jpg  \n","  inflating: /content/new/Hand_0000496.jpg  \n","  inflating: /content/new/Hand_0000497.jpg  \n","  inflating: /content/new/Hand_0000498.jpg  \n","  inflating: /content/new/Hand_0000499.jpg  \n","  inflating: /content/new/Hand_0000500.jpg  \n","  inflating: /content/new/Hand_0000501.jpg  \n","  inflating: /content/new/Hand_0000502.jpg  \n","  inflating: /content/new/Hand_0000503.jpg  \n","  inflating: /content/new/Hand_0000504.jpg  \n","  inflating: /content/new/Hand_0000505.jpg  \n","  inflating: /content/new/Hand_0000507.jpg  \n","  inflating: /content/new/Hand_0000508.jpg  \n","  inflating: /content/new/Hand_0000509.jpg  \n","  inflating: /content/new/Hand_0000510.jpg  \n","  inflating: /content/new/Hand_0000511.jpg  \n","  inflating: /content/new/Hand_0000512.jpg  \n","  inflating: /content/new/Hand_0000513.jpg  \n","  inflating: /content/new/Hand_0000514.jpg  \n","  inflating: /content/new/Hand_0000515.jpg  \n","  inflating: /content/new/Hand_0000516.jpg  \n","  inflating: /content/new/Hand_0000517.jpg  \n","  inflating: /content/new/Hand_0000518.jpg  \n","  inflating: /content/new/Hand_0000519.jpg  \n","  inflating: /content/new/Hand_0000520.jpg  \n","  inflating: /content/new/Hand_0000521.jpg  \n","  inflating: /content/new/Hand_0000522.jpg  \n","  inflating: /content/new/Hand_0000523.jpg  \n","  inflating: /content/new/Hand_0000524.jpg  \n","  inflating: /content/new/Hand_0000525.jpg  \n","  inflating: /content/new/Hand_0000526.jpg  \n","  inflating: /content/new/Hand_0000528.jpg  \n","  inflating: /content/new/Hand_0000529.jpg  \n","  inflating: /content/new/Hand_0000530.jpg  \n","  inflating: /content/new/Hand_0000532.jpg  \n","  inflating: /content/new/Hand_0000538.jpg  \n","  inflating: /content/new/Hand_0000539.jpg  \n","  inflating: /content/new/Hand_0000540.jpg  \n","  inflating: /content/new/Hand_0000541.jpg  \n","  inflating: /content/new/Hand_0000542.jpg  \n","  inflating: /content/new/Hand_0000543.jpg  \n","  inflating: /content/new/Hand_0000545.jpg  \n","  inflating: /content/new/Hand_0000546.jpg  \n","  inflating: /content/new/Hand_0000547.jpg  \n","  inflating: /content/new/Hand_0000548.jpg  \n","  inflating: /content/new/Hand_0000549.jpg  \n","  inflating: /content/new/Hand_0000550.jpg  \n","  inflating: /content/new/Hand_0000551.jpg  \n","  inflating: /content/new/Hand_0000552.jpg  \n","  inflating: /content/new/Hand_0000553.jpg  \n","  inflating: /content/new/Hand_0000554.jpg  \n","  inflating: /content/new/Hand_0000555.jpg  \n","  inflating: /content/new/Hand_0000556.jpg  \n","  inflating: /content/new/Hand_0000559.jpg  \n","  inflating: /content/new/Hand_0000560.jpg  \n","  inflating: /content/new/Hand_0000561.jpg  \n","  inflating: /content/new/Hand_0000562.jpg  \n","  inflating: /content/new/Hand_0000563.jpg  \n","  inflating: /content/new/Hand_0000564.jpg  \n","  inflating: /content/new/Hand_0000565.jpg  \n","  inflating: /content/new/Hand_0000566.jpg  \n","  inflating: /content/new/Hand_0000567.jpg  \n","  inflating: /content/new/Hand_0000568.jpg  \n","  inflating: /content/new/Hand_0000569.jpg  \n","  inflating: /content/new/Hand_0000570.jpg  \n","  inflating: /content/new/Hand_0000571.jpg  \n","  inflating: /content/new/Hand_0000572.jpg  \n","  inflating: /content/new/Hand_0000574.jpg  \n","  inflating: /content/new/Hand_0000575.jpg  \n","  inflating: /content/new/Hand_0000576.jpg  \n","  inflating: /content/new/Hand_0000577.jpg  \n","  inflating: /content/new/Hand_0000578.jpg  \n","  inflating: /content/new/Hand_0000579.jpg  \n","  inflating: /content/new/Hand_0000580.jpg  \n","  inflating: /content/new/Hand_0000581.jpg  \n","  inflating: /content/new/Hand_0000582.jpg  \n","  inflating: /content/new/Hand_0000583.jpg  \n","  inflating: /content/new/Hand_0000584.jpg  \n","  inflating: /content/new/Hand_0000585.jpg  \n","  inflating: /content/new/Hand_0000589.jpg  \n","  inflating: /content/new/Hand_0000590.jpg  \n","  inflating: /content/new/Hand_0000591.jpg  \n","  inflating: /content/new/Hand_0000592.jpg  \n","  inflating: /content/new/Hand_0000594.jpg  \n","  inflating: /content/new/Hand_0000595.jpg  \n","  inflating: /content/new/Hand_0000596.jpg  \n","  inflating: /content/new/Hand_0000597.jpg  \n","  inflating: /content/new/Hand_0000598.jpg  \n","  inflating: /content/new/Hand_0000599.jpg  \n","  inflating: /content/new/Hand_0000600.jpg  \n","  inflating: /content/new/Hand_0000602.jpg  \n","  inflating: /content/new/Hand_0000603.jpg  \n","  inflating: /content/new/Hand_0000604.jpg  \n","  inflating: /content/new/Hand_0000605.jpg  \n","  inflating: /content/new/Hand_0000606.jpg  \n","  inflating: /content/new/Hand_0000607.jpg  \n","  inflating: /content/new/Hand_0000608.jpg  \n","  inflating: /content/new/Hand_0000609.jpg  \n","  inflating: /content/new/Hand_0000610.jpg  \n","  inflating: /content/new/Hand_0000611.jpg  \n","  inflating: /content/new/Hand_0000612.jpg  \n","  inflating: /content/new/Hand_0000614.jpg  \n","  inflating: /content/new/Hand_0000615.jpg  \n","  inflating: /content/new/Hand_0000617.jpg  \n","  inflating: /content/new/Hand_0000618.jpg  \n","  inflating: /content/new/Hand_0000619.jpg  \n","  inflating: /content/new/Hand_0000620.jpg  \n","  inflating: /content/new/Hand_0000621.jpg  \n","  inflating: /content/new/Hand_0000622.jpg  \n","  inflating: /content/new/Hand_0000623.jpg  \n","  inflating: /content/new/Hand_0000624.jpg  \n","  inflating: /content/new/Hand_0000625.jpg  \n","  inflating: /content/new/Hand_0000626.jpg  \n","  inflating: /content/new/Hand_0000627.jpg  \n","  inflating: /content/new/Hand_0000628.jpg  \n","  inflating: /content/new/Hand_0000629.jpg  \n","  inflating: /content/new/Hand_0000630.jpg  \n","  inflating: /content/new/Hand_0000631.jpg  \n","  inflating: /content/new/Hand_0000632.jpg  \n","  inflating: /content/new/Hand_0000633.jpg  \n","  inflating: /content/new/Hand_0000634.jpg  \n","  inflating: /content/new/Hand_0000635.jpg  \n","  inflating: /content/new/Hand_0000636.jpg  \n","  inflating: /content/new/Hand_0000637.jpg  \n","  inflating: /content/new/Hand_0000638.jpg  \n","  inflating: /content/new/Hand_0000639.jpg  \n","  inflating: /content/new/Hand_0000640.jpg  \n","  inflating: /content/new/Hand_0000641.jpg  \n","  inflating: /content/new/Hand_0000643.jpg  \n","  inflating: /content/new/Hand_0000644.jpg  \n","  inflating: /content/new/Hand_0000645.jpg  \n","  inflating: /content/new/Hand_0000646.jpg  \n","  inflating: /content/new/Hand_0000647.jpg  \n","  inflating: /content/new/Hand_0000648.jpg  \n","  inflating: /content/new/Hand_0000649.jpg  \n","  inflating: /content/new/Hand_0000650.jpg  \n","  inflating: /content/new/Hand_0000651.jpg  \n","  inflating: /content/new/Hand_0000652.jpg  \n","  inflating: /content/new/Hand_0000653.jpg  \n","  inflating: /content/new/Hand_0000654.jpg  \n","  inflating: /content/new/Hand_0000655.jpg  \n","  inflating: /content/new/Hand_0000656.jpg  \n","  inflating: /content/new/Hand_0000657.jpg  \n","  inflating: /content/new/Hand_0000658.jpg  \n","  inflating: /content/new/Hand_0000659.jpg  \n","  inflating: /content/new/Hand_0000660.jpg  \n","  inflating: /content/new/Hand_0000661.jpg  \n","  inflating: /content/new/Hand_0000662.jpg  \n","  inflating: /content/new/Hand_0000663.jpg  \n","  inflating: /content/new/Hand_0000664.jpg  \n","  inflating: /content/new/Hand_0000665.jpg  \n","  inflating: /content/new/Hand_0000666.jpg  \n","  inflating: /content/new/Hand_0000667.jpg  \n","  inflating: /content/new/Hand_0000668.jpg  \n","  inflating: /content/new/Hand_0000669.jpg  \n","  inflating: /content/new/Hand_0000672.jpg  \n","  inflating: /content/new/Hand_0000673.jpg  \n","  inflating: /content/new/Hand_0000674.jpg  \n","  inflating: /content/new/Hand_0000675.jpg  \n","  inflating: /content/new/Hand_0000676.jpg  \n","  inflating: /content/new/Hand_0000677.jpg  \n","  inflating: /content/new/Hand_0000678.jpg  \n","  inflating: /content/new/Hand_0000679.jpg  \n","  inflating: /content/new/Hand_0000680.jpg  \n","  inflating: /content/new/Hand_0000681.jpg  \n","  inflating: /content/new/Hand_0000682.jpg  \n","  inflating: /content/new/Hand_0000683.jpg  \n","  inflating: /content/new/Hand_0000684.jpg  \n","  inflating: /content/new/Hand_0000685.jpg  \n","  inflating: /content/new/Hand_0000686.jpg  \n","  inflating: /content/new/Hand_0000687.jpg  \n","  inflating: /content/new/Hand_0000688.jpg  \n","  inflating: /content/new/Hand_0000689.jpg  \n","  inflating: /content/new/Hand_0000694.jpg  \n","  inflating: /content/new/Hand_0000695.jpg  \n","  inflating: /content/new/Hand_0000696.jpg  \n","  inflating: /content/new/Hand_0000697.jpg  \n","  inflating: /content/new/Hand_0000698.jpg  \n","  inflating: /content/new/Hand_0000699.jpg  \n","  inflating: /content/new/Hand_0000700.jpg  \n","  inflating: /content/new/Hand_0000701.jpg  \n","  inflating: /content/new/Hand_0000702.jpg  \n","  inflating: /content/new/Hand_0000703.jpg  \n","  inflating: /content/new/Hand_0000704.jpg  \n","  inflating: /content/new/Hand_0000705.jpg  \n","  inflating: /content/new/Hand_0000706.jpg  \n","  inflating: /content/new/Hand_0000707.jpg  \n","  inflating: /content/new/Hand_0000708.jpg  \n","  inflating: /content/new/Hand_0000709.jpg  \n","  inflating: /content/new/Hand_0000710.jpg  \n","  inflating: /content/new/Hand_0000711.jpg  \n","  inflating: /content/new/Hand_0000712.jpg  \n","  inflating: /content/new/Hand_0000713.jpg  \n","  inflating: /content/new/Hand_0000714.jpg  \n","  inflating: /content/new/Hand_0000715.jpg  \n","  inflating: /content/new/Hand_0000716.jpg  \n","  inflating: /content/new/Hand_0000717.jpg  \n","  inflating: /content/new/Hand_0000718.jpg  \n","  inflating: /content/new/Hand_0000719.jpg  \n","  inflating: /content/new/Hand_0000720.jpg  \n","  inflating: /content/new/Hand_0000721.jpg  \n","  inflating: /content/new/Hand_0000722.jpg  \n","  inflating: /content/new/Hand_0000723.jpg  \n","  inflating: /content/new/Hand_0000724.jpg  \n","  inflating: /content/new/Hand_0000725.jpg  \n","  inflating: /content/new/Hand_0000726.jpg  \n","  inflating: /content/new/Hand_0000727.jpg  \n","  inflating: /content/new/Hand_0000729.jpg  \n","  inflating: /content/new/Hand_0000730.jpg  \n","  inflating: /content/new/Hand_0000731.jpg  \n","  inflating: /content/new/Hand_0000732.jpg  \n","  inflating: /content/new/Hand_0000733.jpg  \n","  inflating: /content/new/Hand_0000734.jpg  \n","  inflating: /content/new/Hand_0000735.jpg  \n","  inflating: /content/new/Hand_0000736.jpg  \n","  inflating: /content/new/Hand_0000737.jpg  \n","  inflating: /content/new/Hand_0000738.jpg  \n","  inflating: /content/new/Hand_0000739.jpg  \n","  inflating: /content/new/Hand_0000740.jpg  \n","  inflating: /content/new/Hand_0000742.jpg  \n","  inflating: /content/new/Hand_0000743.jpg  \n","  inflating: /content/new/Hand_0000744.jpg  \n","  inflating: /content/new/Hand_0000745.jpg  \n","  inflating: /content/new/Hand_0000746.jpg  \n","  inflating: /content/new/Hand_0000748.jpg  \n","  inflating: /content/new/Hand_0000749.jpg  \n","  inflating: /content/new/Hand_0000750.jpg  \n","  inflating: /content/new/Hand_0000751.jpg  \n","  inflating: /content/new/Hand_0000752.jpg  \n","  inflating: /content/new/Hand_0000753.jpg  \n","  inflating: /content/new/Hand_0000754.jpg  \n","  inflating: /content/new/Hand_0000755.jpg  \n","  inflating: /content/new/Hand_0000756.jpg  \n","  inflating: /content/new/Hand_0000757.jpg  \n","  inflating: /content/new/Hand_0000758.jpg  \n","  inflating: /content/new/Hand_0000759.jpg  \n","  inflating: /content/new/Hand_0000760.jpg  \n","  inflating: /content/new/Hand_0000761.jpg  \n","  inflating: /content/new/Hand_0000762.jpg  \n","  inflating: /content/new/Hand_0000763.jpg  \n","  inflating: /content/new/Hand_0000764.jpg  \n","  inflating: /content/new/Hand_0000765.jpg  \n","  inflating: /content/new/Hand_0000766.jpg  \n","  inflating: /content/new/Hand_0000767.jpg  \n","  inflating: /content/new/Hand_0000768.jpg  \n","  inflating: /content/new/Hand_0000769.jpg  \n","  inflating: /content/new/Hand_0000770.jpg  \n","  inflating: /content/new/Hand_0000771.jpg  \n","  inflating: /content/new/Hand_0000774.jpg  \n","  inflating: /content/new/Hand_0000775.jpg  \n","  inflating: /content/new/Hand_0000776.jpg  \n","  inflating: /content/new/Hand_0000777.jpg  \n","  inflating: /content/new/Hand_0000778.jpg  \n","  inflating: /content/new/Hand_0000779.jpg  \n","  inflating: /content/new/Hand_0000780.jpg  \n","  inflating: /content/new/Hand_0000781.jpg  \n","  inflating: /content/new/Hand_0000782.jpg  \n","  inflating: /content/new/Hand_0000783.jpg  \n","  inflating: /content/new/Hand_0000791.jpg  \n","  inflating: /content/new/Hand_0000792.jpg  \n","  inflating: /content/new/Hand_0000793.jpg  \n","  inflating: /content/new/Hand_0000794.jpg  \n","  inflating: /content/new/Hand_0000795.jpg  \n","  inflating: /content/new/Hand_0000796.jpg  \n","  inflating: /content/new/Hand_0000797.jpg  \n","  inflating: /content/new/Hand_0000798.jpg  \n","  inflating: /content/new/Hand_0000805.jpg  \n","  inflating: /content/new/Hand_0000806.jpg  \n","  inflating: /content/new/Hand_0000807.jpg  \n","  inflating: /content/new/Hand_0000809.jpg  \n","  inflating: /content/new/Hand_0000810.jpg  \n","  inflating: /content/new/Hand_0000811.jpg  \n","  inflating: /content/new/Hand_0000812.jpg  \n","  inflating: /content/new/Hand_0000813.jpg  \n","  inflating: /content/new/Hand_0000814.jpg  \n","  inflating: /content/new/Hand_0000815.jpg  \n","  inflating: /content/new/Hand_0000816.jpg  \n","  inflating: /content/new/Hand_0000817.jpg  \n","  inflating: /content/new/Hand_0000818.jpg  \n","  inflating: /content/new/Hand_0000819.jpg  \n","  inflating: /content/new/Hand_0000820.jpg  \n","  inflating: /content/new/Hand_0000821.jpg  \n","  inflating: /content/new/Hand_0000822.jpg  \n","  inflating: /content/new/Hand_0000823.jpg  \n","  inflating: /content/new/Hand_0000824.jpg  \n","  inflating: /content/new/Hand_0000825.jpg  \n","  inflating: /content/new/Hand_0000826.jpg  \n","  inflating: /content/new/Hand_0000827.jpg  \n","  inflating: /content/new/Hand_0000828.jpg  \n","  inflating: /content/new/Hand_0000829.jpg  \n","  inflating: /content/new/Hand_0000830.jpg  \n","  inflating: /content/new/Hand_0000831.jpg  \n","  inflating: /content/new/Hand_0000832.jpg  \n","  inflating: /content/new/Hand_0000833.jpg  \n","  inflating: /content/new/Hand_0000834.jpg  \n","  inflating: /content/new/Hand_0000835.jpg  \n","  inflating: /content/new/Hand_0000836.jpg  \n","  inflating: /content/new/Hand_0000837.jpg  \n","  inflating: /content/new/Hand_0000840.jpg  \n","  inflating: /content/new/Hand_0000841.jpg  \n","  inflating: /content/new/Hand_0000842.jpg  \n","  inflating: /content/new/Hand_0000843.jpg  \n","  inflating: /content/new/Hand_0000844.jpg  \n","  inflating: /content/new/Hand_0000845.jpg  \n","  inflating: /content/new/Hand_0000846.jpg  \n","  inflating: /content/new/Hand_0000847.jpg  \n","  inflating: /content/new/Hand_0000849.jpg  \n","  inflating: /content/new/Hand_0000850.jpg  \n","  inflating: /content/new/Hand_0000851.jpg  \n","  inflating: /content/new/Hand_0000852.jpg  \n","  inflating: /content/new/Hand_0000854.jpg  \n","  inflating: /content/new/Hand_0000855.jpg  \n","  inflating: /content/new/Hand_0000858.jpg  \n","  inflating: /content/new/Hand_0000859.jpg  \n","  inflating: /content/new/Hand_0000860.jpg  \n","  inflating: /content/new/Hand_0000861.jpg  \n","  inflating: /content/new/Hand_0000862.jpg  \n","  inflating: /content/new/Hand_0000863.jpg  \n","  inflating: /content/new/Hand_0000864.jpg  \n","  inflating: /content/new/Hand_0000865.jpg  \n","  inflating: /content/new/Hand_0000866.jpg  \n","  inflating: /content/new/Hand_0000867.jpg  \n","  inflating: /content/new/Hand_0000868.jpg  \n","  inflating: /content/new/Hand_0000869.jpg  \n","  inflating: /content/new/Hand_0000876.jpg  \n","  inflating: /content/new/Hand_0000877.jpg  \n","  inflating: /content/new/Hand_0000878.jpg  \n","  inflating: /content/new/Hand_0000879.jpg  \n","  inflating: /content/new/Hand_0000880.jpg  \n","  inflating: /content/new/Hand_0000881.jpg  \n","  inflating: /content/new/Hand_0000882.jpg  \n","  inflating: /content/new/Hand_0000883.jpg  \n","  inflating: /content/new/Hand_0000884.jpg  \n","  inflating: /content/new/Hand_0000885.jpg  \n","  inflating: /content/new/Hand_0000886.jpg  \n","  inflating: /content/new/Hand_0000887.jpg  \n","  inflating: /content/new/Hand_0000888.jpg  \n","  inflating: /content/new/Hand_0000889.jpg  \n","  inflating: /content/new/Hand_0000890.jpg  \n","  inflating: /content/new/Hand_0000891.jpg  \n","  inflating: /content/new/Hand_0000892.jpg  \n","  inflating: /content/new/Hand_0000893.jpg  \n","  inflating: /content/new/Hand_0000895.jpg  \n","  inflating: /content/new/Hand_0000896.jpg  \n","  inflating: /content/new/Hand_0000897.jpg  \n","  inflating: /content/new/Hand_0000898.jpg  \n","  inflating: /content/new/Hand_0000899.jpg  \n","  inflating: /content/new/Hand_0000900.jpg  \n","  inflating: /content/new/Hand_0000901.jpg  \n","  inflating: /content/new/Hand_0000902.jpg  \n","  inflating: /content/new/Hand_0000903.jpg  \n","  inflating: /content/new/Hand_0000904.jpg  \n","  inflating: /content/new/Hand_0000905.jpg  \n","  inflating: /content/new/Hand_0000906.jpg  \n","  inflating: /content/new/Hand_0000907.jpg  \n","  inflating: /content/new/Hand_0000908.jpg  \n","  inflating: /content/new/Hand_0000909.jpg  \n","  inflating: /content/new/Hand_0000910.jpg  \n","  inflating: /content/new/Hand_0000911.jpg  \n","  inflating: /content/new/Hand_0000912.jpg  \n","  inflating: /content/new/Hand_0000913.jpg  \n","  inflating: /content/new/Hand_0000914.jpg  \n","  inflating: /content/new/Hand_0000915.jpg  \n","  inflating: /content/new/Hand_0000916.jpg  \n","  inflating: /content/new/Hand_0000917.jpg  \n","  inflating: /content/new/Hand_0000918.jpg  \n","  inflating: /content/new/Hand_0000919.jpg  \n","  inflating: /content/new/Hand_0000920.jpg  \n","  inflating: /content/new/Hand_0000921.jpg  \n","  inflating: /content/new/Hand_0000922.jpg  \n","  inflating: /content/new/Hand_0000923.jpg  \n","  inflating: /content/new/Hand_0000924.jpg  \n","  inflating: /content/new/Hand_0000925.jpg  \n","  inflating: /content/new/Hand_0000926.jpg  \n","  inflating: /content/new/Hand_0000927.jpg  \n","  inflating: /content/new/Hand_0000928.jpg  \n","  inflating: /content/new/Hand_0000929.jpg  \n","  inflating: /content/new/Hand_0000930.jpg  \n","  inflating: /content/new/Hand_0000931.jpg  \n","  inflating: /content/new/Hand_0000932.jpg  \n","  inflating: /content/new/Hand_0000933.jpg  \n","  inflating: /content/new/Hand_0000934.jpg  \n","  inflating: /content/new/Hand_0000935.jpg  \n","  inflating: /content/new/Hand_0000936.jpg  \n","  inflating: /content/new/Hand_0000937.jpg  \n","  inflating: /content/new/Hand_0000938.jpg  \n","  inflating: /content/new/Hand_0000941.jpg  \n","  inflating: /content/new/Hand_0000942.jpg  \n","  inflating: /content/new/Hand_0000950.jpg  \n","  inflating: /content/new/Hand_0000951.jpg  \n","  inflating: /content/new/Hand_0000952.jpg  \n","  inflating: /content/new/Hand_0000953.jpg  \n","  inflating: /content/new/Hand_0000954.jpg  \n","  inflating: /content/new/Hand_0000955.jpg  \n","  inflating: /content/new/Hand_0000956.jpg  \n","  inflating: /content/new/Hand_0000957.jpg  \n","  inflating: /content/new/Hand_0000958.jpg  \n","  inflating: /content/new/Hand_0000959.jpg  \n","  inflating: /content/new/Hand_0000960.jpg  \n","  inflating: /content/new/Hand_0000961.jpg  \n","  inflating: /content/new/Hand_0000962.jpg  \n","  inflating: /content/new/Hand_0000963.jpg  \n","  inflating: /content/new/Hand_0000964.jpg  \n","  inflating: /content/new/Hand_0000965.jpg  \n","  inflating: /content/new/Hand_0000966.jpg  \n","  inflating: /content/new/Hand_0000970.jpg  \n","  inflating: /content/new/Hand_0000971.jpg  \n","  inflating: /content/new/Hand_0000972.jpg  \n","  inflating: /content/new/Hand_0000973.jpg  \n","  inflating: /content/new/Hand_0000974.jpg  \n","  inflating: /content/new/Hand_0000976.jpg  \n","  inflating: /content/new/Hand_0000977.jpg  \n","  inflating: /content/new/Hand_0000978.jpg  \n","  inflating: /content/new/Hand_0000979.jpg  \n","  inflating: /content/new/Hand_0000980.jpg  \n","  inflating: /content/new/Hand_0000981.jpg  \n","  inflating: /content/new/Hand_0000982.jpg  \n","  inflating: /content/new/Hand_0000983.jpg  \n","  inflating: /content/new/Hand_0000984.jpg  \n","  inflating: /content/new/Hand_0000985.jpg  \n","  inflating: /content/new/Hand_0000986.jpg  \n","  inflating: /content/new/Hand_0000987.jpg  \n","  inflating: /content/new/Hand_0000988.jpg  \n","  inflating: /content/new/Hand_0000989.jpg  \n","  inflating: /content/new/Hand_0000990.jpg  \n","  inflating: /content/new/Hand_0000991.jpg  \n","  inflating: /content/new/Hand_0000992.jpg  \n","  inflating: /content/new/Hand_0000993.jpg  \n","  inflating: /content/new/Hand_0000994.jpg  \n","  inflating: /content/new/Hand_0000995.jpg  \n","  inflating: /content/new/Hand_0000996.jpg  \n","  inflating: /content/new/Hand_0000997.jpg  \n","  inflating: /content/new/Hand_0000998.jpg  \n","  inflating: /content/new/Hand_0000999.jpg  \n","  inflating: /content/new/Hand_0001000.jpg  \n","  inflating: /content/new/Hand_0001001.jpg  \n","  inflating: /content/new/Hand_0001002.jpg  \n","  inflating: /content/new/Hand_0001003.jpg  \n","  inflating: /content/new/Hand_0001004.jpg  \n","  inflating: /content/new/Hand_0001005.jpg  \n","  inflating: /content/new/Hand_0001006.jpg  \n","  inflating: /content/new/Hand_0001007.jpg  \n","  inflating: /content/new/Hand_0001008.jpg  \n","  inflating: /content/new/Hand_0001009.jpg  \n","  inflating: /content/new/Hand_0001012.jpg  \n","  inflating: /content/new/Hand_0001013.jpg  \n","  inflating: /content/new/Hand_0001014.jpg  \n","  inflating: /content/new/Hand_0001015.jpg  \n","  inflating: /content/new/Hand_0001016.jpg  \n","  inflating: /content/new/Hand_0001017.jpg  \n","  inflating: /content/new/Hand_0001018.jpg  \n","  inflating: /content/new/Hand_0001019.jpg  \n","  inflating: /content/new/Hand_0001020.jpg  \n","  inflating: /content/new/Hand_0001021.jpg  \n","  inflating: /content/new/Hand_0001022.jpg  \n","  inflating: /content/new/Hand_0001023.jpg  \n","  inflating: /content/new/Hand_0001024.jpg  \n","  inflating: /content/new/Hand_0001032.jpg  \n","  inflating: /content/new/Hand_0001033.jpg  \n","  inflating: /content/new/Hand_0001034.jpg  \n","  inflating: /content/new/Hand_0001035.jpg  \n","  inflating: /content/new/Hand_0001036.jpg  \n","  inflating: /content/new/Hand_0001037.jpg  \n","  inflating: /content/new/Hand_0001038.jpg  \n","  inflating: /content/new/Hand_0001039.jpg  \n","  inflating: /content/new/Hand_0001040.jpg  \n","  inflating: /content/new/Hand_0001041.jpg  \n","  inflating: /content/new/Hand_0001042.jpg  \n","  inflating: /content/new/Hand_0001043.jpg  \n","  inflating: /content/new/Hand_0001045.jpg  \n","  inflating: /content/new/Hand_0001046.jpg  \n","  inflating: /content/new/Hand_0001047.jpg  \n","  inflating: /content/new/Hand_0001048.jpg  \n","  inflating: /content/new/Hand_0001049.jpg  \n","  inflating: /content/new/Hand_0001051.jpg  \n","  inflating: /content/new/Hand_0001052.jpg  \n","  inflating: /content/new/Hand_0001053.jpg  \n","  inflating: /content/new/Hand_0001054.jpg  \n","  inflating: /content/new/Hand_0001055.jpg  \n","  inflating: /content/new/Hand_0001056.jpg  \n","  inflating: /content/new/Hand_0001057.jpg  \n","  inflating: /content/new/Hand_0001058.jpg  \n","  inflating: /content/new/Hand_0001063.jpg  \n","  inflating: /content/new/Hand_0001064.jpg  \n","  inflating: /content/new/Hand_0001065.jpg  \n","  inflating: /content/new/Hand_0001066.jpg  \n","  inflating: /content/new/Hand_0001067.jpg  \n","  inflating: /content/new/Hand_0001068.jpg  \n","  inflating: /content/new/Hand_0001069.jpg  \n","  inflating: /content/new/Hand_0001071.jpg  \n","  inflating: /content/new/Hand_0001072.jpg  \n","  inflating: /content/new/Hand_0001073.jpg  \n","  inflating: /content/new/Hand_0001074.jpg  \n","  inflating: /content/new/Hand_0001075.jpg  \n","  inflating: /content/new/Hand_0001076.jpg  \n","  inflating: /content/new/Hand_0001077.jpg  \n","  inflating: /content/new/Hand_0001078.jpg  \n","  inflating: /content/new/Hand_0001079.jpg  \n","  inflating: /content/new/Hand_0001080.jpg  \n","  inflating: /content/new/Hand_0001081.jpg  \n","  inflating: /content/new/Hand_0001082.jpg  \n","  inflating: /content/new/Hand_0001083.jpg  \n","  inflating: /content/new/Hand_0001084.jpg  \n","  inflating: /content/new/Hand_0001085.jpg  \n","  inflating: /content/new/Hand_0001086.jpg  \n","  inflating: /content/new/Hand_0001087.jpg  \n","  inflating: /content/new/Hand_0001088.jpg  \n","  inflating: /content/new/Hand_0001089.jpg  \n","  inflating: /content/new/Hand_0001090.jpg  \n","  inflating: /content/new/Hand_0001091.jpg  \n","  inflating: /content/new/Hand_0001092.jpg  \n","  inflating: /content/new/Hand_0001093.jpg  \n","  inflating: /content/new/Hand_0001094.jpg  \n","  inflating: /content/new/Hand_0001095.jpg  \n","  inflating: /content/new/Hand_0001096.jpg  \n","  inflating: /content/new/Hand_0001097.jpg  \n","  inflating: /content/new/Hand_0001098.jpg  \n","  inflating: /content/new/Hand_0001099.jpg  \n","  inflating: /content/new/Hand_0001101.jpg  \n","  inflating: /content/new/Hand_0001102.jpg  \n","  inflating: /content/new/Hand_0001103.jpg  \n","  inflating: /content/new/Hand_0001104.jpg  \n","  inflating: /content/new/Hand_0001105.jpg  \n","  inflating: /content/new/Hand_0001106.jpg  \n","  inflating: /content/new/Hand_0001107.jpg  \n","  inflating: /content/new/Hand_0001108.jpg  \n","  inflating: /content/new/Hand_0001109.jpg  \n","  inflating: /content/new/Hand_0001110.jpg  \n","  inflating: /content/new/Hand_0001111.jpg  \n","  inflating: /content/new/Hand_0001112.jpg  \n","  inflating: /content/new/Hand_0001113.jpg  \n","  inflating: /content/new/Hand_0001114.jpg  \n","  inflating: /content/new/Hand_0001115.jpg  \n","  inflating: /content/new/Hand_0001116.jpg  \n","  inflating: /content/new/Hand_0001118.jpg  \n","  inflating: /content/new/Hand_0001119.jpg  \n","  inflating: /content/new/Hand_0001124.jpg  \n","  inflating: /content/new/Hand_0001126.jpg  \n","  inflating: /content/new/Hand_0001127.jpg  \n","  inflating: /content/new/Hand_0001128.jpg  \n","  inflating: /content/new/Hand_0001134.jpg  \n","  inflating: /content/new/Hand_0001135.jpg  \n","  inflating: /content/new/Hand_0001136.jpg  \n","  inflating: /content/new/Hand_0001137.jpg  \n","  inflating: /content/new/Hand_0001138.jpg  \n","  inflating: /content/new/Hand_0001139.jpg  \n","  inflating: /content/new/Hand_0001140.jpg  \n","  inflating: /content/new/Hand_0001141.jpg  \n","  inflating: /content/new/Hand_0001142.jpg  \n","  inflating: /content/new/Hand_0001143.jpg  \n","  inflating: /content/new/Hand_0001144.jpg  \n","  inflating: /content/new/Hand_0001145.jpg  \n","  inflating: /content/new/Hand_0001149.jpg  \n","  inflating: /content/new/Hand_0001150.jpg  \n","  inflating: /content/new/Hand_0001151.jpg  \n","  inflating: /content/new/Hand_0001152.jpg  \n","  inflating: /content/new/Hand_0001153.jpg  \n","  inflating: /content/new/Hand_0001154.jpg  \n","  inflating: /content/new/Hand_0001155.jpg  \n","  inflating: /content/new/Hand_0001156.jpg  \n","  inflating: /content/new/Hand_0001157.jpg  \n","  inflating: /content/new/Hand_0001158.jpg  \n","  inflating: /content/new/Hand_0001159.jpg  \n","  inflating: /content/new/Hand_0001160.jpg  \n","  inflating: /content/new/Hand_0001161.jpg  \n","  inflating: /content/new/Hand_0001162.jpg  \n","  inflating: /content/new/Hand_0001163.jpg  \n","  inflating: /content/new/Hand_0001164.jpg  \n","  inflating: /content/new/Hand_0001165.jpg  \n","  inflating: /content/new/Hand_0001166.jpg  \n","  inflating: /content/new/Hand_0001167.jpg  \n","  inflating: /content/new/Hand_0001168.jpg  \n","  inflating: /content/new/Hand_0001169.jpg  \n","  inflating: /content/new/Hand_0001170.jpg  \n","  inflating: /content/new/Hand_0001171.jpg  \n","  inflating: /content/new/Hand_0001172.jpg  \n","  inflating: /content/new/Hand_0001173.jpg  \n","  inflating: /content/new/Hand_0001174.jpg  \n","  inflating: /content/new/Hand_0001175.jpg  \n","  inflating: /content/new/Hand_0001176.jpg  \n","  inflating: /content/new/Hand_0001177.jpg  \n","  inflating: /content/new/Hand_0001178.jpg  \n","  inflating: /content/new/Hand_0001179.jpg  \n","  inflating: /content/new/Hand_0001180.jpg  \n","  inflating: /content/new/Hand_0001181.jpg  \n","  inflating: /content/new/Hand_0001182.jpg  \n","  inflating: /content/new/Hand_0001183.jpg  \n","  inflating: /content/new/Hand_0001184.jpg  \n","  inflating: /content/new/Hand_0001185.jpg  \n","  inflating: /content/new/Hand_0001186.jpg  \n","  inflating: /content/new/Hand_0001187.jpg  \n","  inflating: /content/new/Hand_0001188.jpg  \n","  inflating: /content/new/Hand_0001189.jpg  \n","  inflating: /content/new/Hand_0001190.jpg  \n","  inflating: /content/new/Hand_0001191.jpg  \n","  inflating: /content/new/Hand_0001192.jpg  \n","  inflating: /content/new/Hand_0001193.jpg  \n","  inflating: /content/new/Hand_0001194.jpg  \n","  inflating: /content/new/Hand_0001195.jpg  \n","  inflating: /content/new/Hand_0001196.jpg  \n","  inflating: /content/new/Hand_0001197.jpg  \n","  inflating: /content/new/Hand_0001198.jpg  \n","  inflating: /content/new/Hand_0001199.jpg  \n","  inflating: /content/new/Hand_0001200.jpg  \n","  inflating: /content/new/Hand_0001201.jpg  \n","  inflating: /content/new/Hand_0001202.jpg  \n","  inflating: /content/new/Hand_0001203.jpg  \n","  inflating: /content/new/Hand_0001204.jpg  \n"]}],"execution_count":null},{"cell_type":"code","source":"!accelerate launch /kaggle/input/asda_awdq/pytorch/default/1/sdxl_finetune_lora.py --train_data_dir /kaggle/input/hands-112391/new --train_batch_size 1 --mixed_precision fp16 --resolution 512  --max_train_steps=\"50000\"\n","metadata":{"id":"_kcmWRro1iGx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05211cb3-80de-463d-d790-9c9ae8b645b4","_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:24:03.084443Z","iopub.execute_input":"2024-12-12T10:24:03.084831Z"}},"outputs":[{"name":"stdout","text":"You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n{'beta_end', 'beta_start', 'rescale_betas_zero_snr', 'trained_betas', 'steps_offset', 'variance_type', 'clip_sample_range', 'timestep_spacing', 'beta_schedule', 'clip_sample'} was not found in config. Values will be initialized to default values.\n{'use_quant_conv', 'shift_factor', 'mid_block_add_attention', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\ndiffusion_pytorch_model.safetensors: 100%|█| 10.3G/10.3G [03:24<00:00, 42.0MB/s]\nResolving data files: 100%|██████████████| 1001/1001 [00:00<00:00, 13040.35it/s]\nResolving data files: 100%|██████████████| 1001/1001 [00:00<00:00, 42356.02it/s]\nDownloading data: 100%|███████████████| 1001/1001 [00:00<00:00, 58922.98files/s]\nGenerating train split: 1000 examples [00:00, 8489.91 examples/s]\nSteps:   0%|                                          | 0/50000 [00:00<?, ?it/s][rank1]:[W1212 10:28:13.297932119 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[rank0]:[W1212 10:28:13.299727651 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\nSteps:   1%| | 500/50000 [07:52<12:40:20,  1.09it/s, lr=0.0001, step_loss=0.618]Model weights saved in sd-model-finetuned-lora/checkpoint-500/pytorch_lora_weights.safetensors\nSteps:   2%| | 1000/50000 [15:46<12:59:49,  1.05it/s, lr=0.0001, step_loss=0.756Model weights saved in sd-model-finetuned-lora/checkpoint-1000/pytorch_lora_weights.safetensors\nSteps:   3%| | 1500/50000 [23:39<12:31:30,  1.08it/s, lr=0.0001, step_loss=0.394Model weights saved in sd-model-finetuned-lora/checkpoint-1500/pytorch_lora_weights.safetensors\nSteps:   4%| | 2000/50000 [31:33<12:17:51,  1.08it/s, lr=0.0001, step_loss=0.829Model weights saved in sd-model-finetuned-lora/checkpoint-2000/pytorch_lora_weights.safetensors\nSteps:   5%| | 2500/50000 [39:28<12:19:24,  1.07it/s, lr=0.0001, step_loss=0.315Model weights saved in sd-model-finetuned-lora/checkpoint-2500/pytorch_lora_weights.safetensors\nSteps:   6%| | 3000/50000 [47:21<12:13:36,  1.07it/s, lr=0.0001, step_loss=0.783Model weights saved in sd-model-finetuned-lora/checkpoint-3000/pytorch_lora_weights.safetensors\nSteps:   7%|▏ | 3500/50000 [55:14<12:52:09,  1.00it/s, lr=0.0001, step_loss=0.7]Model weights saved in sd-model-finetuned-lora/checkpoint-3500/pytorch_lora_weights.safetensors\nSteps:   8%| | 4000/50000 [1:03:05<11:42:28,  1.09it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-4000/pytorch_lora_weights.safetensors\nSteps:   9%| | 4500/50000 [1:10:58<11:30:19,  1.10it/s, lr=0.0001, step_loss=0.9Model weights saved in sd-model-finetuned-lora/checkpoint-4500/pytorch_lora_weights.safetensors\nSteps:  10%| | 5000/50000 [1:18:50<11:28:33,  1.09it/s, lr=0.0001, step_loss=0.3Model weights saved in sd-model-finetuned-lora/checkpoint-5000/pytorch_lora_weights.safetensors\nSteps:  11%| | 5500/50000 [1:26:43<11:14:07,  1.10it/s, lr=0.0001, step_loss=0.3Model weights saved in sd-model-finetuned-lora/checkpoint-5500/pytorch_lora_weights.safetensors\nSteps:  12%| | 6000/50000 [1:34:33<11:13:12,  1.09it/s, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-6000/pytorch_lora_weights.safetensors\nSteps:  13%|▏| 6500/50000 [1:42:27<11:16:37,  1.07it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-6500/pytorch_lora_weights.safetensors\nSteps:  14%|▏| 7000/50000 [1:50:18<11:14:35,  1.06it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-7000/pytorch_lora_weights.safetensors\nSteps:  15%|▏| 7500/50000 [1:58:11<11:22:48,  1.04it/s, lr=0.0001, step_loss=0.9Model weights saved in sd-model-finetuned-lora/checkpoint-7500/pytorch_lora_weights.safetensors\nSteps:  16%|▏| 8000/50000 [2:06:04<11:11:02,  1.04it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-8000/pytorch_lora_weights.safetensors\nSteps:  17%|▏| 8500/50000 [2:13:59<10:34:44,  1.09it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-8500/pytorch_lora_weights.safetensors\nSteps:  18%|▏| 9000/50000 [2:21:51<10:29:01,  1.09it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-9000/pytorch_lora_weights.safetensors\nSteps:  19%|▏| 9500/50000 [2:29:42<10:17:46,  1.09it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-9500/pytorch_lora_weights.safetensors\nSteps:  20%|▏| 10000/50000 [2:37:37<10:25:47,  1.07it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-10000/pytorch_lora_weights.safetensors\nSteps:  21%|▏| 10500/50000 [2:45:30<10:12:20,  1.08it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-10500/pytorch_lora_weights.safetensors\nSteps:  22%|▏| 11000/50000 [2:53:22<9:54:33,  1.09it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-11000/pytorch_lora_weights.safetensors\nSteps:  23%|▏| 11500/50000 [3:01:16<10:04:51,  1.06it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-11500/pytorch_lora_weights.safetensors\nSteps:  24%|▏| 12000/50000 [3:09:12<9:56:56,  1.06it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-12000/pytorch_lora_weights.safetensors\nSteps:  25%|▎| 12500/50000 [3:17:10<9:51:01,  1.06it/s, lr=0.0001, step_loss=0.9Model weights saved in sd-model-finetuned-lora/checkpoint-12500/pytorch_lora_weights.safetensors\nSteps:  26%|▎| 13000/50000 [3:25:06<9:38:05,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-13000/pytorch_lora_weights.safetensors\nSteps:  27%|▎| 13500/50000 [3:33:03<9:26:04,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-13500/pytorch_lora_weights.safetensors\nSteps:  28%|▎| 14000/50000 [3:41:06<9:19:14,  1.07it/s, lr=0.0001, step_loss=0.3Model weights saved in sd-model-finetuned-lora/checkpoint-14000/pytorch_lora_weights.safetensors\nSteps:  29%|▎| 14500/50000 [3:49:01<9:10:26,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-14500/pytorch_lora_weights.safetensors\nSteps:  30%|▎| 15000/50000 [3:56:56<8:59:18,  1.08it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-15000/pytorch_lora_weights.safetensors\nSteps:  31%|▎| 15500/50000 [4:04:50<8:46:11,  1.09it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-15500/pytorch_lora_weights.safetensors\nSteps:  32%|▎| 16000/50000 [4:12:42<8:47:47,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-16000/pytorch_lora_weights.safetensors\nSteps:  33%|▎| 16500/50000 [4:20:34<9:06:37,  1.02it/s, lr=0.0001, step_loss=0.5Model weights saved in sd-model-finetuned-lora/checkpoint-16500/pytorch_lora_weights.safetensors\nSteps:  34%|▎| 17000/50000 [4:28:26<8:20:16,  1.10it/s, lr=0.0001, step_loss=0.7Model weights saved in sd-model-finetuned-lora/checkpoint-17000/pytorch_lora_weights.safetensors\nSteps:  35%|▎| 17500/50000 [4:36:19<8:28:12,  1.07it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-17500/pytorch_lora_weights.safetensors\nSteps:  36%|▎| 18000/50000 [4:44:13<8:10:34,  1.09it/s, lr=0.0001, step_loss=0.5Model weights saved in sd-model-finetuned-lora/checkpoint-18000/pytorch_lora_weights.safetensors\nSteps:  37%|▎| 18500/50000 [4:52:09<8:08:45,  1.07it/s, lr=0.0001, step_loss=0.7Model weights saved in sd-model-finetuned-lora/checkpoint-18500/pytorch_lora_weights.safetensors\nSteps:  38%|▍| 19000/50000 [5:00:04<8:01:30,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-19000/pytorch_lora_weights.safetensors\nSteps:  39%|▍| 19500/50000 [5:07:59<7:47:35,  1.09it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-19500/pytorch_lora_weights.safetensors\nSteps:  40%|▍| 20000/50000 [5:15:52<7:44:46,  1.08it/s, lr=0.0001, step_loss=0.3Model weights saved in sd-model-finetuned-lora/checkpoint-20000/pytorch_lora_weights.safetensors\nSteps:  41%|▍| 20500/50000 [5:23:47<7:40:09,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-20500/pytorch_lora_weights.safetensors\nSteps:  42%|▍| 21000/50000 [5:31:39<7:24:05,  1.09it/s, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-21000/pytorch_lora_weights.safetensors\nSteps:  43%|▍| 21500/50000 [5:39:33<7:24:08,  1.07it/s, lr=0.0001, step_loss=0.5Model weights saved in sd-model-finetuned-lora/checkpoint-21500/pytorch_lora_weights.safetensors\nSteps:  44%|▍| 22000/50000 [5:47:26<7:10:50,  1.08it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-22000/pytorch_lora_weights.safetensors\nSteps:  45%|▍| 22500/50000 [5:55:19<7:05:00,  1.08it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-22500/pytorch_lora_weights.safetensors\nSteps:  46%|▍| 23000/50000 [6:03:11<7:35:39,  1.01s/it, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-23000/pytorch_lora_weights.safetensors\nSteps:  47%|▍| 23500/50000 [6:11:04<6:52:34,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-23500/pytorch_lora_weights.safetensors\nSteps:  48%|▍| 24000/50000 [6:18:58<6:44:26,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-24000/pytorch_lora_weights.safetensors\nSteps:  49%|▍| 24500/50000 [6:26:53<6:37:43,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-24500/pytorch_lora_weights.safetensors\nSteps:  50%|▌| 25000/50000 [6:34:48<6:26:45,  1.08it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-25000/pytorch_lora_weights.safetensors\nSteps:  51%|▌| 25500/50000 [6:42:40<6:20:37,  1.07it/s, lr=0.0001, step_loss=0.8Model weights saved in sd-model-finetuned-lora/checkpoint-25500/pytorch_lora_weights.safetensors\nSteps:  52%|▌| 26000/50000 [6:50:32<6:29:11,  1.03it/s, lr=0.0001, step_loss=0.3Model weights saved in sd-model-finetuned-lora/checkpoint-26000/pytorch_lora_weights.safetensors\nSteps:  53%|▌| 26500/50000 [6:58:23<5:55:45,  1.10it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-26500/pytorch_lora_weights.safetensors\nSteps:  54%|▌| 27000/50000 [7:06:18<5:54:18,  1.08it/s, lr=0.0001, step_loss=0.7Model weights saved in sd-model-finetuned-lora/checkpoint-27000/pytorch_lora_weights.safetensors\nSteps:  55%|▌| 27500/50000 [7:14:11<5:50:35,  1.07it/s, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-27500/pytorch_lora_weights.safetensors\nSteps:  56%|▌| 28000/50000 [7:22:06<5:40:45,  1.08it/s, lr=0.0001, step_loss=0.0Model weights saved in sd-model-finetuned-lora/checkpoint-28000/pytorch_lora_weights.safetensors\nSteps:  57%|▌| 28500/50000 [7:30:00<5:34:25,  1.07it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-28500/pytorch_lora_weights.safetensors\nSteps:  58%|▌| 29000/50000 [7:37:53<5:33:41,  1.05it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-29000/pytorch_lora_weights.safetensors\nSteps:  59%|▌| 29500/50000 [7:45:46<5:17:15,  1.08it/s, lr=0.0001, step_loss=0.7Model weights saved in sd-model-finetuned-lora/checkpoint-29500/pytorch_lora_weights.safetensors\nSteps:  60%|▌| 30000/50000 [7:53:41<5:10:21,  1.07it/s, lr=0.0001, step_loss=0.8Model weights saved in sd-model-finetuned-lora/checkpoint-30000/pytorch_lora_weights.safetensors\nSteps:  61%|▌| 30500/50000 [8:01:32<4:55:24,  1.10it/s, lr=0.0001, step_loss=0.8Model weights saved in sd-model-finetuned-lora/checkpoint-30500/pytorch_lora_weights.safetensors\nSteps:  62%|▌| 31000/50000 [8:09:24<4:52:45,  1.08it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-31000/pytorch_lora_weights.safetensors\nSteps:  63%|▋| 31500/50000 [8:17:17<4:47:48,  1.07it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-31500/pytorch_lora_weights.safetensors\nSteps:  64%|▋| 32000/50000 [8:25:11<5:00:33,  1.00s/it, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-32000/pytorch_lora_weights.safetensors\nSteps:  65%|▋| 32500/50000 [8:33:05<4:30:26,  1.08it/s, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-32500/pytorch_lora_weights.safetensors\nSteps:  66%|▋| 33000/50000 [8:40:59<4:24:03,  1.07it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-33000/pytorch_lora_weights.safetensors\nSteps:  67%|▋| 33500/50000 [8:48:51<4:12:10,  1.09it/s, lr=0.0001, step_loss=0.6Model weights saved in sd-model-finetuned-lora/checkpoint-33500/pytorch_lora_weights.safetensors\nSteps:  68%|▋| 34000/50000 [8:56:45<4:05:17,  1.09it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-34000/pytorch_lora_weights.safetensors\nSteps:  69%|▋| 34500/50000 [9:04:37<4:08:56,  1.04it/s, lr=0.0001, step_loss=0.1Model weights saved in sd-model-finetuned-lora/checkpoint-34500/pytorch_lora_weights.safetensors\nSteps:  70%|▋| 35000/50000 [9:12:29<3:48:40,  1.09it/s, lr=0.0001, step_loss=0.0Model weights saved in sd-model-finetuned-lora/checkpoint-35000/pytorch_lora_weights.safetensors\nSteps:  71%|▋| 35500/50000 [9:20:23<3:42:56,  1.08it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-35500/pytorch_lora_weights.safetensors\nSteps:  72%|▋| 36000/50000 [9:28:16<3:35:27,  1.08it/s, lr=0.0001, step_loss=0.0Model weights saved in sd-model-finetuned-lora/checkpoint-36000/pytorch_lora_weights.safetensors\nSteps:  73%|▋| 36500/50000 [9:36:09<3:28:59,  1.08it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-36500/pytorch_lora_weights.safetensors\nSteps:  74%|▋| 37000/50000 [9:44:03<3:40:05,  1.02s/it, lr=0.0001, step_loss=0.8Model weights saved in sd-model-finetuned-lora/checkpoint-37000/pytorch_lora_weights.safetensors\nSteps:  75%|▊| 37500/50000 [9:51:56<3:28:08,  1.00it/s, lr=0.0001, step_loss=0.2Model weights saved in sd-model-finetuned-lora/checkpoint-37500/pytorch_lora_weights.safetensors\nSteps:  76%|▊| 38000/50000 [9:59:48<3:06:39,  1.07it/s, lr=0.0001, step_loss=0.4Model weights saved in sd-model-finetuned-lora/checkpoint-38000/pytorch_lora_weights.safetensors\nSteps:  77%|▊| 38500/50000 [10:07:44<2:57:15,  1.08it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-38500/pytorch_lora_weights.safetensors\nSteps:  78%|▊| 39000/50000 [10:15:40<2:49:53,  1.08it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-39000/pytorch_lora_weights.safetensors\nSteps:  79%|▊| 39500/50000 [10:23:34<2:44:12,  1.07it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-39500/pytorch_lora_weights.safetensors\nSteps:  80%|▊| 40000/50000 [10:31:28<2:39:33,  1.04it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-40000/pytorch_lora_weights.safetensors\nSteps:  81%|▊| 40500/50000 [10:39:22<2:33:24,  1.03it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-40500/pytorch_lora_weights.safetensors\nSteps:  82%|▊| 41000/50000 [10:47:14<2:32:27,  1.02s/it, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-41000/pytorch_lora_weights.safetensors\nSteps:  83%|▊| 41500/50000 [10:55:05<2:10:28,  1.09it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-41500/pytorch_lora_weights.safetensors\nSteps:  84%|▊| 42000/50000 [11:02:59<2:04:18,  1.07it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-42000/pytorch_lora_weights.safetensors\nSteps:  85%|▊| 42500/50000 [11:10:53<1:56:55,  1.07it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-42500/pytorch_lora_weights.safetensors\nSteps:  86%|▊| 43000/50000 [11:18:46<1:48:03,  1.08it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-43000/pytorch_lora_weights.safetensors\nSteps:  87%|▊| 43500/50000 [11:26:40<1:42:33,  1.06it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-43500/pytorch_lora_weights.safetensors\nSteps:  88%|▉| 44000/50000 [11:34:32<1:32:09,  1.09it/s, lr=0.0001, step_loss=0.Model weights saved in sd-model-finetuned-lora/checkpoint-44000/pytorch_lora_weights.safetensors\nSteps:  89%|▉| 44373/50000 [11:40:25<1:27:30,  1.07it/s, lr=0.0001, step_loss=0.","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionXLPipeline\nimport torch\n\n# Path to your base model and LoRA weights\nbase_model = \"playgroundai/playground-v2.5-1024px-aesthetic\"\n# lora_weights_path = \"./lora-output\"\n\n# Load the base pipeline\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    base_model,\n    torch_dtype=torch.float16,\n    variant=\"fp16\"\n).to(\"cuda\")\n\n# Load the LoRA weights\npipeline.load_lora_weights(\"/kaggle/input/checpoint-16500/pytorch/default/1\")\n\n# Define your prompt\nprompt = \"generate a picture of a dog smiling\"\nnum_inference_steps = 50\nguidance_scale = 7.5\n\n# Generate the image\nwith torch.autocast(\"cuda\"):\n    image = pipeline(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images[0]\n\n# Save the resulting image\nimage.save(\"generated_image2.png\")\nprint(\"Image saved to generated_image2.png\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["388de842b7a644b4a0869d0689bc8e9b","92b3d9052dca4b6cbc972b150543d131","547da6ff19924beb8b8e4baf6977c916","f62243cfa3af4cd3bb7016cbe5668949","0b03526485314680950c7b2d5bf64a9f","5163cc045eb94813b1884dc817e4dc5e","c177517a63e44721ac50b38987e6c8ac","0fc1f56bcf0f406cb0890ee3ee9b4bc2","eaad4d69f208441bb97499f857ff5a40","714f40149efe417ba8d4526f89ce2e1b","d07e1c359d3f4f52a88df737ddc9eb45","c3af97dcc70d45f6bc01aeaabc1b7873","47586a87e4c44574982bd27e48394e51","c8d398ed8470420db7e7efbc78cd1af1","c5645182e0714f0880f704e300c2f5f4","9c49f2c23bf44a93b403e2a1145aae3e","4ef2f2771bb84b859998eec25c0ab9c7","d91951f2eb1f459682aee7d78d7bda06","2c3842b2dbe54b788acabb46665f6a4f","6be2f7ccbee948659bbfef40e937614f","a8858a8968024442b5637d676f14bb40","26a08cde44914b778aceb3489c8ea276"]},"id":"24QDMw0l_rW1","outputId":"7d9730a6-5c40-4023-b10e-9525015ad218","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:32:24.506692Z","iopub.execute_input":"2024-12-13T05:32:24.507030Z","iopub.status.idle":"2024-12-13T05:32:28.438710Z","shell.execute_reply.started":"2024-12-13T05:32:24.507002Z","shell.execute_reply":"2024-12-13T05:32:28.437544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe88f185a704d869d5f012db0876cd5"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m StableDiffusionXLPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     base_model,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m     variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the LoRA weights\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/checpoint-16500/pytorch/default/1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Define your prompt\u001b[39;00m\n\u001b[1;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate a picture of a dog smiling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/loaders/lora_pipeline.py:628\u001b[0m, in \u001b[0;36mStableDiffusionXLLoraLoaderMixin.load_lora_weights\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03mLoad LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m`self.text_encoder`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEFT backend is required for this method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    630\u001b[0m low_cpu_mem_usage \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow_cpu_mem_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m, _LOW_CPU_MEM_USAGE_DEFAULT_LORA)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.13.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mValueError\u001b[0m: PEFT backend is required for this method."],"ename":"ValueError","evalue":"PEFT backend is required for this method.","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"!pip install --upgrade peft\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T05:30:16.189058Z","iopub.execute_input":"2024-12-13T05:30:16.189450Z","iopub.status.idle":"2024-12-13T05:30:24.587141Z","shell.execute_reply.started":"2024-12-13T05:30:16.189409Z","shell.execute_reply":"2024-12-13T05:30:24.586243Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"Hello world\")","metadata":{"id":"1yl1MjMrSeN1","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:12:38.681610Z","iopub.execute_input":"2024-12-13T04:12:38.681861Z","iopub.status.idle":"2024-12-13T04:12:38.703248Z","shell.execute_reply.started":"2024-12-13T04:12:38.681827Z","shell.execute_reply":"2024-12-13T04:12:38.702434Z"}},"outputs":[{"name":"stdout","text":"Hello world\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}